{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Using_GloVe_Embeddings_For_Summarization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ2o-edy_qKV",
        "outputId": "f0b194d1-78fe-435f-bbcb-c03a9060d068"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import sent_tokenize\n",
        "from tensorflow import keras \n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9D_uVdG_5Rs",
        "outputId": "89ecd8e5-f8a3-4ad0-fafd-d3228b60fe34"
      },
      "source": [
        "# Mount GDrive \n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "4pU6CDnk_5Tx",
        "outputId": "3b989788-929b-4e1d-ca9d-3a540be9f7e0"
      },
      "source": [
        "data = pd.read_csv(\"/gdrive/MyDrive/Summarizer/structured_df.csv\", index_col = 0)\n",
        "data.head()"
      ],
      "execution_count": 363,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>course_title</th>\n",
              "      <th>section_topic</th>\n",
              "      <th>subsection_topic</th>\n",
              "      <th>lecture_topic</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>advanced-methods-reinforcement-learning-finance</td>\n",
              "      <td>other-applications-of-reinforcement-learning-p...</td>\n",
              "      <td>lesson-1</td>\n",
              "      <td>trades-quotes-and-order-flow</td>\n",
              "      <td>All right. Let's first talk about what type of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>advanced-methods-reinforcement-learning-finance</td>\n",
              "      <td>other-applications-of-reinforcement-learning-p...</td>\n",
              "      <td>lesson-1</td>\n",
              "      <td>electronic-markets-and-lob</td>\n",
              "      <td>So, so far in this specialization our examples...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>advanced-methods-reinforcement-learning-finance</td>\n",
              "      <td>other-applications-of-reinforcement-learning-p...</td>\n",
              "      <td>lesson-1</td>\n",
              "      <td>welcome</td>\n",
              "      <td>Welcome to week four of our course on events t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>advanced-methods-reinforcement-learning-finance</td>\n",
              "      <td>other-applications-of-reinforcement-learning-p...</td>\n",
              "      <td>lesson-1</td>\n",
              "      <td>limit-order-book</td>\n",
              "      <td>Now, let's talk a bit more about the Limit Ord...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>advanced-methods-reinforcement-learning-finance</td>\n",
              "      <td>other-applications-of-reinforcement-learning-p...</td>\n",
              "      <td>lesson-1</td>\n",
              "      <td>lob-modeling</td>\n",
              "      <td>Now, after we talked about what the LOB is, le...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                      course_title  ...                                            content\n",
              "0  advanced-methods-reinforcement-learning-finance  ...  All right. Let's first talk about what type of...\n",
              "1  advanced-methods-reinforcement-learning-finance  ...  So, so far in this specialization our examples...\n",
              "2  advanced-methods-reinforcement-learning-finance  ...  Welcome to week four of our course on events t...\n",
              "3  advanced-methods-reinforcement-learning-finance  ...  Now, let's talk a bit more about the Limit Ord...\n",
              "4  advanced-methods-reinforcement-learning-finance  ...  Now, after we talked about what the LOB is, le...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 363
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVKM923hTtko"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "texts = data.content.values\n",
        "all_sentences = [sent_tokenize(t) for t in texts]\n",
        "#sentences = sent_tokenize(text)"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vcooSRaTtm5"
      },
      "source": [
        "flat_list = [item for sublist in all_sentences for item in sublist]"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqJthqm4jan8"
      },
      "source": [
        "### Creating the Tokenizer and Tokenizing all the sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emSZgKSTWP1l"
      },
      "source": [
        "# Creating the Tokenizer and Tokenizing all the sentences\n",
        "tokenizer = Tokenizer( oov_token=\"[UNK]\")\n",
        "tokenizer.fit_on_texts(flat_list)\n",
        "word_index = tokenizer.word_index\n",
        "inv_word_index = {v: k for k, v in word_index.items()}\n",
        "sequences = tokenizer.texts_to_sequences(flat_list)"
      ],
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLHXnmZGy9sL"
      },
      "source": [
        "word_index[\"\"]=0\n",
        "inv_word_index[0]=\"\""
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU9_QWh4zE-l"
      },
      "source": [
        "vocab = len(word_index)"
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv9rDIgZq_Pa"
      },
      "source": [
        "### Creating the word tokenized form of all of the sentences so that we can form the word embedding matrix of each of the texts "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vte9Uj7Ttqr",
        "outputId": "3465d341-9e7e-4653-ed6a-ba60ed761ea0"
      },
      "source": [
        "# Creating the word tokenization of all of the sentences and storing it \n",
        "all_sent_word_list = []\n",
        "for j in sequences: \n",
        "  word_list = [inv_word_index[i] for i in j]\n",
        "  all_sent_word_list.append(word_list)\n",
        "\n",
        "all_sent_word_list[:2]\n"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['all', 'right'],\n",
              " [\"let's\",\n",
              "  'first',\n",
              "  'talk',\n",
              "  'about',\n",
              "  'what',\n",
              "  'type',\n",
              "  'of',\n",
              "  'data',\n",
              "  'is',\n",
              "  'available',\n",
              "  'within',\n",
              "  'order',\n",
              "  'driven',\n",
              "  'market']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIvXr04njiGI"
      },
      "source": [
        "### Finding the distribution of sentence lengths so that we can Pad Sequences "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbHIMQxHgRSP",
        "outputId": "2940b55c-e927-470d-e1ea-d9d770bde6c0"
      },
      "source": [
        "# Removing Erroneous Entries where the entire paragraph is a sentence due to error in data format\n",
        "sent_lengths = []\n",
        "for i in all_sent_word_list:\n",
        "  current_len = len(i)\n",
        "  if current_len <300:\n",
        "    sent_lengths.append(current_len)\n",
        "  else:\n",
        "    print(\"Removing entry {1} with contents {0}\".format(i, all_sent_word_list.index(i)))\n",
        "    all_sent_word_list.remove(i)\n"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Removing entry 134379 with contents ['hi', 'there', \"i'm\", 'david', 'dye', 'and', \"i'll\", 'be', 'taking', 'through', 'the', 'last', 'few', 'modules', 'of', 'this', 'course', 'in', 'this', 'module', \"we'll\", 'start', 'to', 'use', 'the', 'calculus', \"we've\", 'done', 'and', 'put', 'it', 'together', 'with', 'vectors', 'in', 'order', 'to', 'start', 'solving', 'equations', 'in', 'this', 'first', 'video', \"we'll\", 'look', 'at', 'a', 'nice', 'simple', 'case', 'where', 'we', 'just', 'need', 'to', 'find', 'the', 'gradient', 'the', 'derivative', 'in', 'order', 'to', 'solve', 'an', 'equation', 'using', \"what's\", 'called', 'the', 'newton', 'raphson', 'method', 'now', 'say', \"we've\", 'got', 'that', 'distribution', 'of', 'heights', 'again', 'with', 'a', 'mean', 'an', 'average', 'mu', 'and', 'a', 'width', 'sigma', 'and', 'we', 'want', 'to', 'fit', 'an', 'equation', 'to', 'that', 'distribution', 'that', 'so', 'we', \"don't\", 'have', 'to', 'if', 'we', 'fitted', 'it', 'bother', 'about', 'carrying', 'around', 'all', 'the', 'data', 'points', 'we', 'just', 'have', 'a', 'model', 'with', 'two', 'parameters', 'a', 'mean', 'and', 'a', 'width', 'and', 'we', 'could', 'do', 'everything', 'using', 'just', 'the', 'model', 'and', 'that', 'would', 'be', 'loads', 'faster', 'and', 'simpler', 'and', 'would', 'let', 'us', 'make', 'predictions', 'and', 'so', 'on', 'so', \"it'd\", 'be', 'much', 'much', 'nicer', 'but', 'how', 'do', 'we', 'find', 'the', 'right', 'parameters', 'for', 'the', 'model', 'how', 'do', 'we', 'find', 'the', 'best', 'mu', 'and', 'sigma', 'we', 'can', 'well', 'what', \"we're\", 'gonna', 'do', 'is', 'going', 'to', 'find', 'some', 'expression', 'for', 'how', 'well', 'the', 'model', 'fits', 'the', 'data', 'and', 'then', 'look', 'at', 'how', 'that', 'goodness', 'of', 'fit', 'varies', 'is', 'the', 'fitting', 'parameters', 'mu', 'and', 'sigma', 'vary', 'so', 'trying', 'to', 'solve', 'an', 'equation', 'where', 'the', 'fitting', 'prompts', 'the', 'variables', 'in', 'the', 'equation', 'but', 'in', 'order', 'to', 'get', 'there', 'in', 'the', 'next', 'module', 'actually', \"we're\", 'first', 'gonna', 'needs', 'to', 'a', 'bit', 'more', 'calculus', 'so', 'first', \"let's\", 'look', 'at', 'the', 'equation', 'of', 'a', 'line', 'so', 'here', 'y', 'equals', 'x', 'cubed', 'minus', '2x', 'plus', '2', 'if', 'we', 'differentiate', 'this', 'equation', 'we', 'get', 'the', 'quadratic', '3x', 'squared', 'minus', '2', 'and', 'that', 'quadratic', 'will', 'have', 'two', 'solutions', 'and', 'therefore', 'two', 'turning', 'points', 'will', 'exist', 'one', 'a', 'maximum', 'one', 'a', 'minimum', 'just', 'as', 'we', 'see', 'here', 'now', 'say', 'that', 'i', \"don't\", 'know', 'what', 'the', 'equation', 'looks', 'like', \"i'm\", 'blind', 'and', 'i', \"haven't\", 'got', 'enough', 'computer', 'resources', 'to', 'graph', 'out', 'the', 'values', 'at', 'every', 'point', 'or', 'more', 'likely', 'in', 'reality', 'say', 'the', 'function', 'exists', 'in', 'so', 'many', 'dimensions', 'that', 'i', \"can't\", 'visualize', 'it', 'at', 'all', 'but', \"it's\", 'say', 'i', 'only', 'need', 'to', 'find', 'the', 'solution', 'to', 'the', 'equation', 'where', 'y', 'equals', 'nor', 'so', 'where', 'x', 'cubed', 'minus', '2x', 'plus', '2', 'is', 'equal', 'to', '0', 'we', 'can', 'see', 'that', \"it's\", 'actually', 'only', 'one', 'solution', 'here', 'on', 'this', 'graph', 'but', 'there', 'could', 'be', 'more', 'depending', 'on', 'the', 'exact', 'form', 'of', 'the', 'equation', 'i', 'was', 'trying', 'to', 'solve', 'now', 'say', 'that', 'i', 'have', 'an', 'idea', 'that', 'i', 'want', 'to', 'hunt', 'from', 'solutions', 'somewhere', 'near', 'some', 'initial', 'guess', 'at', 'the', 'red', 'dot', 'here', 'for', 'instance', 'the', 'constants', 'pretty', 'small', 'and', 'positive', 'so', 'my', 'guess', 'is', 'i', 'need', 'a', 'slightly', 'negative', 'value', 'of', 'xa', 'minus', '2', 'might', 'be', 'somewhere', 'near', 'a', 'solution', 'now', 'if', 'i', 'can', 'evaluate', 'the', 'value', 'of', 'the', 'function', 'at', 'my', 'guess', 'of', 'x', 'equals', '2', 'i', 'find', 'that', 'the', 'function', 'has', 'a', 'value', 'of', 'minus', '2', 'and', 'if', 'i', 'ask', 'what', 'the', 'gradient', 'is', 'at', 'that', 'value', 'of', 'x', 'equals', '2', 'i', 'find', 'that', 'the', 'gradient', 'is', 'positive', 'and', \"it's\", '10', 'now', 'i', 'can', 'extrapolate', 'the', 'gradient', 'to', 'the', 'intersect', 'with', 'the', 'y', 'axis', 'which', 'is', 'would', 'be', 'my', 'first', 'guess', 'of', 'the', 'solution', 'of', 'the', 'equation', 'i', 'was', 'trying', 'to', 'solve', 'where', 'it', 'finds', 'that', 'intercept', 'with', 'the', 'y', 'axis', 'so', 'i', 'can', 'use', 'that', 'value', 'of', 'x', 'at', 'the', 'intercept', 'as', 'a', 'new', 'estimate', 'for', 'what', 'the', 'solution', 'to', 'the', 'equation', 'is', 'effectively', \"i'm\", 'guessing', 'that', 'the', 'function', 'is', 'a', 'straight', 'line', 'and', 'then', \"i'm\", 'using', 'the', 'gradient', 'to', 'extrapolate', 'to', 'find', 'the', 'solution', 'it', \"isn't\", 'really', 'a', 'straight', 'line', 'of', 'course', 'but', 'that', 'first', 'order', 'approximation', 'would', 'be', 'that', \"it's\", 'a', 'straight', 'line', 'and', \"we'll\", 'use', 'that', 'to', 'update', 'our', 'guess', 'and', 'then', 'go', 'again', 'and', 'evaluate', 'so', 'i', 'can', 'write', 'down', 'an', 'equation', 'for', 'my', 'new', 'guess', 'x', 'i', 'plus', '1', 'based', 'on', 'my', 'previous', 'guess', 'and', \"it's\", 'an', 'x', 'i', 'as', 'being', 'my', 'original', 'guess', 'minus', 'the', 'value', 'of', 'the', 'function', 'divided', 'by', 'its', 'gradient', 'so', \"let's\", 'see', 'how', 'it', 'plays', 'out', 'we', 'can', 'make', 'a', 'table', 'starting', 'with', 'our', 'initial', 'guess', 'that', 'i', 'equals', 'naught', 'and', 'then', 'we', 'can', 'find', 'the', 'gradient', 'and', 'the', 'intercept', 'and', 'then', 'use', 'that', 'to', 'generate', 'a', 'new', 'guess', 'in', 'this', 'case', 'minus', '2', 'minus', '2', 'divided', 'by', '10', 'gives', 'us', 'minus', '2', 'plus', '0', '2', 'which', 'is', 'minus', '1', '8', 'then', 'we', 'can', 'evaluate', 'the', 'result', 'for', 'that', 'guess', 'and', 'find', 'that', \"it's\", 'just', 'a', 'little', 'less', 'than', 'zero', 'nor', 'point', 'two', 'three', 'and', 'the', 'gradient', 'is', 'seven', 'point', 'seven', 'so', \"we've\", 'gone', 'from', 'being', 'out', 'by', '2', 'on', 'y', 'to', 'being', 'out', 'by', 'just', 'naught', 'point', 'two', 'three', 'so', 'in', 'some', 'sense', \"we've\", 'got', 'like', '10', 'times', 'better', 'a', 'nice', 'timet', 'just', 'in', 'our', 'first', 'go', 'if', 'we', 'carry', 'on', 'then', 'we', 'get', 'the', 'next', 'guess', 'for', 'x2', 'is', 'minus', '1', 'point', '7', '7', 'and', \"that's\", 'just', 'naught', 'point', 'naught', '5', 'away', 'from', 'the', 'axis', \"it's\", 'really', 'close', 'then', 'if', 'we', 'have', 'another', 'go', 'after', 'just', '3', 'iterations', 'we', 'get', 'an', 'answer', 'of', 'x', 'equals', 'minus', 'one', 'point', 'seven', 'six', 'nine', 'which', 'is', 'just', 'two', 'point', 'three', 'times', 'ten', 'to', 'the', 'minus', 'six', 'the', 'axis', 'so', 'in', 'just', 'three', 'iterations', 'we', 'pretty', 'much', 'solve', 'the', 'problem', 'which', 'is', 'pretty', 'cool', 'this', 'method', 'is', 'called', 'the', 'newton', 'raphson', 'method', 'and', 'its', 'really', 'pretty', 'neat', 'to', 'solve', 'an', 'equation', 'all', 'we', 'need', 'to', 'be', 'able', 'to', 'do', 'is', 'evaluate', 'it', 'and', 'differentiate', 'it', 'we', \"don't\", 'need', 'to', 'graph', 'and', 'visualize', 'it', 'everywhere', 'calculating', 'it', 'lots', 'and', 'lots', 'of', 'times', 'we', \"don't\", 'need', 'to', 'be', 'able', 'to', 'solve', 'it', 'algebraically', 'either', 'which', 'if', 'we', 'have', 'lots', 'of', 'dimensions', 'to', 'a', 'dataset', 'say', 'and', 'a', 'big', 'multi', 'multi', 'variable', 'function', \"we're\", 'trying', 'to', 'fit', 'to', 'that', 'data', \"it's\", 'going', 'to', 'be', 'much', 'too', 'expensive', 'to', 'try', 'and', 'solve', 'it', 'analytically', 'or', 'even', 'plot', 'it', 'out', 'for', 'all', 'the', 'possible', 'values', 'of', 'the', 'variables', 'this', 'sort', 'of', 'method', 'where', 'we', 'try', 'a', 'solution', 'and', 'evaluate', 'it', 'and', 'then', 'generate', 'a', 'new', 'guess', 'and', 'then', 'evaluate', 'that', 'and', 'again', 'and', 'again', 'and', 'again', \"it's\", 'called', 'iteration', 'and', \"it's\", 'a', 'very', 'fundamental', 'computational', 'approach', 'now', 'there', 'are', 'some', 'things', 'that', 'can', 'go', 'wrong', 'sometimes', 'with', 'this', 'method', 'so', \"let's\", 'briefly', 'look', 'at', 'those', 'say', 'i', 'started', 'off', 'with', 'a', 'guess', 'of', 'x', 'equals', '0', 'which', 'evaluates', 'to', 'y', 'equals', '2', 'when', 'i', 'find', 'the', 'gradient', 'for', 'that', 'and', 'extrapolate', 'it', 'and', 'takes', 'me', 'away', 'from', 'the', 'solution', 'to', 'the', 'other', 'side', 'of', 'the', 'turning', 'point', 'it', 'gives', 'me', 'a', 'new', 'guess', 'that', 'x', 'equals', '1', 'what', 'i', 'evaluate', 'that', 'then', 'i', 'get', 'a', 'value', 'for', 'y', 'at', 'x', 'equals', '1', 'of', '1', 'when', 'i', 'find', 'the', 'gradient', 'and', 'extrapolate', 'back', 'then', 'my', 'new', 'estimate', 'lands', 'me', 'back', 'at', 'x', 'equals', '0', 'just', 'where', 'i', 'begun', 'so', 'i', 'have', 'a', 'problem', 'i', 'seem', 'to', 'a', 'magically', 'landed', 'in', 'a', 'closed', 'loop', 'where', 'my', 'estimates', 'just', 'cycle', 'back', 'between', 'x', 'equals', 'naught', 'and', 'x', 'equals', '1', 'and', 'i', 'never', 'get', 'close', 'i', 'never', 'even', 'go', 'anywhere', 'near', 'to', 'the', 'solution', 'x', 'equals', 'minus', '1', 'point', '7', '6', '9', \"there's\", 'another', 'problem', 'which', 'is', 'that', 'if', \"i'm\", 'close', 'to', 'a', 'turning', 'point', 'this', 'bottom', 'here', 'to', 'a', 'minimum', 'or', 'a', 'maximum', 'then', 'because', 'my', 'gradient', 'will', 'be', 'very', 'small', 'when', 'i', 'divide', 'by', 'the', 'gradient', 'in', 'the', 'newton', 'raphson', 'equation', 'here', 'my', 'next', 'estimate', 'will', 'take', 'me', 'zapping', 'off', 'to', 'some', 'crazy', 'value', 'and', 'therefore', 'it', \"won't\", 'converges', 'there', 'dive', 'off', 'somewhere', 'those', 'are', 'the', 'problems', 'so', \"that's\", 'the', 'newton', 'raphson', 'method', 'we', 'iterate', 'to', 'a', 'solution', 'to', 'an', 'equation', 'by', 'each', 'time', 'making', 'a', 'new', 'estimate', 'from', 'the', 'solution', 'using', 'the', 'gradient', 'to', 'extrapolate', 'or', 'the', 'solution', 'then', 'going', 'again', 'and', 'again', 'and', 'again', 'a', 'most', 'of', 'the', 'time', 'this', 'works', 'really', 'well', 'as', 'a', 'means', 'to', 'step', 'towards', 'the', 'solution', 'so', 'what', \"we've\", 'done', 'in', 'this', 'video', 'is', 'look', 'at', 'a', 'method', 'for', 'using', 'just', 'the', 'gradient', 'to', 'step', 'our', 'way', 'towards', 'solving', 'a', 'problem', 'this', 'method', 'is', 'called', 'the', 'newton', 'raphson', 'method', 'and', \"it's\", 'a', 'really', 'powerful', 'way', 'to', 'solve', 'an', 'equation', 'just', 'by', 'evaluating', 'it', 'and', 'its', 'gradient', 'a', 'few', 'times', \"it's\", 'as', 'if', \"you're\", 'standing', 'on', 'a', 'hill', 'in', 'the', 'fog', 'and', 'you', 'can', 'know', 'your', 'height', 'and', 'you', 'can', 'know', 'locally', \"what's\", 'going', 'on', 'around', 'you', 'can', 'know', 'how', 'steep', 'the', 'hill', 'is', 'but', 'you', \"can't\", 'see', 'the', 'whole', 'landscape', 'around', 'you', 'you', \"don't\", 'know', 'what', 'it', 'looks', 'like', 'you', \"don't\", 'know', 'how', 'to', 'get', 'down', 'the', 'mountain', 'if', 'you', 'like', 'down', 'to', 'a', 'nice', 'safe', 'place', 'that', \"doesn't\", 'have', 'cliffs', 'so', 'what', 'you', 'do', 'is', 'you', 'guess', 'based', 'on', 'how', 'steep', 'the', 'hill', 'is', 'locally', 'around', 'you', 'which', 'way', 'to', 'go', 'that', 'you', 'want', 'to', 'go', 'down', 'to', 'sea', 'level', 'so', 'you', 'go', 'down', 'the', 'hill', 'then', 'you', 'take', 'that', 'step', 'blindfolded', 'and', 'when', 'you', 'get', 'there', 'you', 'ask', 'again', 'what', 'height', 'you', 'out', 'and', 'how', 'steep', 'it', 'is', 'and', 'then', 'you', 'keep', 'making', 'more', 'steps', 'down', 'the', 'hill', 'until', 'either', 'something', 'goes', 'wrong', 'and', 'you', 'need', 'to', 'go', 'back', 'down', 'the', 'other', 'way', 'or', 'you', 'get', 'home', 'to', 'where', 'you', 'want', 'it', 'want', 'it', 'to', 'be', 'the', 'point', 'is', 'you', \"don't\", 'need', 'to', 'know', 'what', 'the', 'landscape', 'looks', 'like', 'the', 'function', 'you', 'just', 'need', 'an', 'altimeter', 'the', 'value', 'of', 'the', 'function', 'and', 'to', 'be', 'able', 'to', 'feel', 'with', 'your', 'toe', 'what', 'the', 'gradient', 'is', 'like', 'locally', 'around', 'you', 'what', \"we'll\", 'do', 'in', 'the', 'next', 'video', 'is', 'look', 'at', 'how', 'to', 'apply', 'this', 'where', \"we've\", 'got', 'multiple', 'variables', 'not', 'just', 'x', 'and', 'that', 'will', 'involve', 'finding', 'the', 'gradient', 'vector', 'how', 'to', 'go', 'down', 'a', 'hill', 'on', 'a', 'contour', 'plot']\n",
            "Removing entry 142806 with contents ['if', 'the', 'basic', 'technical', 'idea', 'is', 'behind', 'deep', 'learning', 'behind', 'your', 'networks', 'have', 'been', 'around', 'for', 'decades', 'why', 'are', 'they', 'only', 'just', 'now', 'taking', 'off', 'in', 'this', 'video', \"let's\", 'go', 'over', 'some', 'of', 'the', 'main', 'drivers', 'behind', 'the', 'rise', 'of', 'deep', 'learning', 'because', 'i', 'think', 'this', 'will', 'help', 'you', 'that', 'the', 'spot', 'the', 'best', 'opportunities', 'within', 'your', 'own', 'organization', 'to', 'apply', 'these', 'to', 'over', 'the', 'last', 'few', 'years', 'a', 'lot', 'of', 'people', 'have', 'asked', 'me', 'andrew', 'why', 'is', 'deep', 'learning', 'certainly', 'working', 'so', 'well', 'and', 'when', 'a', 'marsan', 'question', 'this', 'is', 'usually', 'the', 'picture', 'i', 'draw', 'for', 'them', \"let's\", 'say', 'we', 'plot', 'a', 'figure', 'where', 'on', 'the', 'horizontal', 'axis', 'we', 'plot', 'the', 'amount', 'of', 'data', 'we', 'have', 'for', 'a', 'task', 'and', \"let's\", 'say', 'on', 'the', 'vertical', 'axis', 'we', 'plot', 'the', 'performance', 'on', 'above', 'learning', 'algorithms', 'such', 'as', 'the', 'accuracy', 'of', 'our', 'spam', 'classifier', 'or', 'our', 'ad', 'click', 'predictor', 'or', 'the', 'accuracy', 'of', 'our', 'neural', 'net', 'for', 'figuring', 'out', 'the', 'position', 'of', 'other', 'calls', 'for', 'our', 'self', 'driving', 'car', 'it', 'turns', 'out', 'if', 'you', 'plot', 'the', 'performance', 'of', 'a', 'traditional', 'learning', 'algorithm', 'like', 'support', 'vector', 'machine', 'or', 'logistic', 'regression', 'as', 'a', 'function', 'of', 'the', 'amount', 'of', 'data', 'you', 'have', 'you', 'might', 'get', 'a', 'curve', 'that', 'looks', 'like', 'this', 'where', 'the', 'performance', 'improves', 'for', 'a', 'while', 'as', 'you', 'add', 'more', 'data', 'but', 'after', 'a', 'while', 'the', 'performance', 'you', 'know', 'pretty', 'much', 'plateaus', 'right', 'suppose', 'your', 'horizontal', 'lines', 'enjoy', 'that', 'very', 'well', 'you', 'know', 'was', 'it', 'they', \"didn't\", 'know', 'what', 'to', 'do', 'with', 'huge', 'amounts', 'of', 'data', 'and', 'what', 'happened', 'in', 'our', 'society', 'over', 'the', 'last', '10', 'years', 'maybe', 'is', 'that', 'for', 'a', 'lot', 'of', 'problems', 'we', 'went', 'from', 'having', 'a', 'relatively', 'small', 'amount', 'of', 'data', 'to', 'having', 'you', 'know', 'often', 'a', 'fairly', 'large', 'amount', 'of', 'data', 'and', 'all', 'of', 'this', 'was', 'thanks', 'to', 'the', 'digitization', 'of', 'a', 'society', 'where', 'so', 'much', 'human', 'activity', 'is', 'now', 'in', 'the', 'digital', 'realm', 'we', 'spend', 'so', 'much', 'time', 'on', 'the', 'computers', 'on', 'websites', 'on', 'mobile', 'apps', 'and', 'activities', 'on', 'digital', 'devices', 'creates', 'data', 'and', 'thanks', 'to', 'the', 'rise', 'of', 'inexpensive', 'cameras', 'built', 'into', 'our', 'cell', 'phones', 'accelerometers', 'all', 'sorts', 'of', 'sensors', 'in', 'the', 'internet', 'of', 'things', 'we', 'also', 'just', 'have', 'been', 'collecting', 'one', 'more', 'and', 'more', 'data', 'so', 'over', 'the', 'last', '20', 'years', 'for', 'a', 'lot', 'of', 'applications', 'we', 'just', 'accumulate', 'a', 'lot', 'more', 'data', 'more', 'than', 'traditional', 'learning', 'algorithms', 'were', 'able', 'to', 'effectively', 'take', 'advantage', 'of', 'and', 'what', 'new', 'network', 'lead', 'turns', 'out', 'that', 'if', 'you', 'train', 'a', 'small', 'neural', 'net', 'then', 'this', 'performance', 'maybe', 'looks', 'like', 'that', 'if', 'you', 'train', 'a', 'somewhat', 'larger', 'internet', \"that's\", 'called', 'as', 'a', 'medium', 'sized', 'internet', 'to', 'fall', 'in', 'something', 'a', 'little', 'bit', 'better', 'and', 'if', 'you', 'train', 'a', 'very', 'large', 'neural', 'net', 'then', \"it's\", 'the', 'form', 'and', 'often', 'just', 'keeps', 'getting', 'better', 'and', 'better', 'so', 'couple', 'observations', 'one', 'is', 'if', 'you', 'want', 'to', 'hit', 'this', 'very', 'high', 'level', 'of', 'performance', 'then', 'you', 'need', 'two', 'things', 'first', 'often', 'you', 'need', 'to', 'be', 'able', 'to', 'train', 'a', 'big', 'enough', 'neural', 'network', 'in', 'order', 'to', 'take', 'advantage', 'of', 'the', 'huge', 'amount', 'of', 'data', 'and', 'second', 'you', 'need', 'to', 'be', 'out', 'here', 'on', 'the', 'x', 'axes', 'you', 'do', 'need', 'a', 'lot', 'of', 'data', 'so', 'we', 'often', 'say', 'that', 'scale', 'has', 'been', 'driving', 'deep', 'learning', 'progress', 'and', 'by', 'scale', 'i', 'mean', 'both', 'the', 'size', 'of', 'the', 'neural', 'network', 'we', 'need', 'just', 'a', 'new', 'network', 'a', 'lot', 'of', 'hidden', 'units', 'a', 'lot', 'of', 'parameters', 'a', 'lot', 'of', 'connections', 'as', 'well', 'as', 'scale', 'of', 'the', 'data', 'in', 'fact', 'today', 'one', 'of', 'the', 'most', 'reliable', 'ways', 'to', 'get', 'better', 'performance', 'in', 'the', 'neural', 'network', 'is', 'often', 'to', 'either', 'train', 'a', 'bigger', 'network', 'or', 'throw', 'more', 'data', 'at', 'it', 'and', 'that', 'only', 'works', 'up', 'to', 'a', 'point', 'because', 'eventually', 'you', 'run', 'out', 'of', 'data', 'or', 'eventually', 'then', 'your', 'network', 'is', 'so', 'big', 'that', 'it', 'takes', 'too', 'long', 'to', 'train', 'but', 'just', 'improving', 'scale', 'has', 'actually', 'taken', 'us', 'a', 'long', 'way', 'in', 'the', 'world', 'of', 'learning', 'in', 'order', 'to', 'make', 'this', 'diagram', 'a', 'bit', 'more', 'technically', 'precise', 'and', 'just', 'add', 'a', 'few', 'more', 'things', 'i', 'wrote', 'the', 'amount', 'of', 'data', 'on', 'the', 'x', 'axis', 'technically', 'this', 'is', 'amount', 'of', 'labeled', 'data', 'where', 'by', 'label', 'data', 'i', 'mean', 'training', 'examples', 'we', 'have', 'both', 'the', 'input', 'x', 'and', 'the', 'label', 'y', 'i', 'went', 'to', 'introduce', 'a', 'little', 'bit', 'of', 'notation', 'that', \"we'll\", 'use', 'later', 'in', 'this', 'course', \"we're\", 'going', 'to', 'use', 'lowercase', 'alphabet', 'to', 'denote', 'the', 'size', 'of', 'my', 'training', 'sets', 'or', 'the', 'number', 'of', 'training', 'examples', 'this', 'lowercase', 'm', 'so', \"that's\", 'the', 'horizontal', 'axis', 'couple', 'other', 'details', 'to', 'this', 'tigger', 'in', 'this', 'regime', 'of', 'smaller', 'training', 'sets', 'the', 'relative', 'ordering', 'of', 'the', 'algorithms', 'is', 'actually', 'not', 'very', 'well', 'defined', 'so', 'if', 'you', \"don't\", 'have', 'a', 'lot', 'of', 'training', 'data', 'is', 'often', 'up', 'to', 'your', 'skill', 'at', 'hand', 'engineering', 'features', 'that', 'determines', 'the', 'foreman', 'so', \"it's\", 'quite', 'possible', 'that', 'if', 'someone', 'training', 'an', 'svm', 'is', 'more', 'motivated', 'to', 'hand', 'engineer', 'features', 'and', 'someone', 'training', 'even', 'large', 'their', 'own', 'that', 'may', 'be', 'in', 'this', 'small', 'training', 'set', 'regime', 'the', 'sem', 'could', 'do', 'better', 'so', 'you', 'know', 'in', 'this', 'region', 'to', 'the', 'left', 'of', 'the', 'figure', 'the', 'relative', 'ordering', 'between', 'gene', 'algorithms', 'is', 'not', 'that', 'well', 'defined', 'and', 'performance', 'depends', 'much', 'more', 'on', 'your', 'skill', 'at', 'engine', 'features', 'and', 'other', 'mobile', 'details', 'of', 'the', 'algorithms', 'and', \"there's\", 'only', 'in', 'this', 'some', 'big', 'data', 'regime', 'very', 'large', 'training', 'sets', 'very', 'large', 'm', 'regime', 'in', 'the', 'right', 'that', 'we', 'more', 'consistently', 'see', 'largely', 'ronettes', 'dominating', 'the', 'other', 'approaches', 'and', 'so', 'if', 'any', 'of', 'your', 'friends', 'ask', 'you', 'why', 'are', 'known', 'as', 'you', 'know', 'taking', 'off', 'i', 'would', 'encourage', 'you', 'to', 'draw', 'this', 'picture', 'for', 'them', 'as', 'well', 'so', 'i', 'will', 'say', 'that', 'in', 'the', 'early', 'days', 'in', 'their', 'modern', 'rise', 'of', 'deep', 'learning', 'it', 'was', 'scaled', 'data', 'and', 'scale', 'of', 'computation', 'just', 'our', 'ability', 'to', 'train', 'very', 'large', 'dinner', 'networks', 'either', 'on', 'a', 'cpu', 'or', 'gpu', 'that', 'enabled', 'us', 'to', 'make', 'a', 'lot', 'of', 'progress', 'but', 'increasingly', 'especially', 'in', 'the', 'last', 'several', 'years', \"we've\", 'seen', 'tremendous', 'algorithmic', 'innovation', 'as', 'well', 'so', 'i', 'also', \"don't\", 'want', 'to', 'understate', 'that', 'interestingly', 'many', 'of', 'the', 'algorithmic', 'innovations', 'have', 'been', 'about', 'trying', 'to', 'make', 'neural', 'networks', 'run', 'much', 'faster', 'so', 'as', 'a', 'concrete', 'example', 'one', 'of', 'the', 'huge', 'breakthroughs', 'in', 'your', 'networks', 'has', 'been', 'switching', 'from', 'a', 'sigmoid', 'function', 'which', 'looks', 'like', 'this', 'to', 'a', 'railer', 'function', 'which', 'we', 'talked', 'about', 'briefly', 'in', 'an', 'early', 'video', 'that', 'looks', 'like', 'this', 'if', 'you', \"don't\", 'understand', 'the', 'details', 'of', 'one', 'about', 'the', 'state', \"don't\", 'worry', 'about', 'it', 'but', 'it', 'turns', 'out', 'that', 'one', 'of', 'the', 'problems', 'of', 'using', 'sigmoid', 'functions', 'and', 'machine', 'learning', 'is', 'that', 'there', 'these', 'regions', 'here', 'where', 'the', 'slope', 'of', 'the', 'function', 'would', 'gradient', 'is', 'nearly', 'zero', 'and', 'so', 'learning', 'becomes', 'really', 'slow', 'because', 'when', 'you', 'implement', 'gradient', 'descent', 'and', 'gradient', 'is', 'zero', 'the', 'parameters', 'just', 'change', 'very', 'slowly', 'and', 'so', 'learning', 'is', 'very', 'slow', 'whereas', 'by', 'changing', 'the', \"what's\", 'called', 'the', 'activation', 'function', 'the', 'neural', 'network', 'to', 'use', 'this', 'function', 'called', 'the', 'value', 'function', 'of', 'the', 'rectified', 'linear', 'unit', 'our', 'elu', 'the', 'gradient', 'is', 'equal', 'to', 'one', 'for', 'all', 'positive', 'values', 'of', 'input', 'right', 'and', 'so', 'the', 'gradient', 'is', 'much', 'less', 'likely', 'to', 'gradually', 'shrink', 'to', 'zero', 'and', 'the', 'gradient', 'here', 'the', 'slope', 'of', 'this', 'line', 'is', 'zero', 'on', 'the', 'left', 'but', 'it', 'turns', 'out', 'that', 'just', 'by', 'switching', 'to', 'the', 'sigmoid', 'function', 'to', 'the', 'rayleigh', 'function', 'has', 'made', 'an', 'algorithm', 'called', 'gradient', 'descent', 'work', 'much', 'faster', 'and', 'so', 'this', 'is', 'an', 'example', 'of', 'maybe', 'relatively', 'simple', 'algorithm', 'in', 'bayesian', 'but', 'ultimately', 'the', 'impact', 'of', 'this', 'algorithmic', 'innovation', 'was', 'it', 'really', 'hope', 'computation', 'so', 'the', 'regimen', 'quite', 'a', 'lot', 'of', 'examples', 'like', 'this', 'of', 'where', 'we', 'change', 'the', 'algorithm', 'because', 'it', 'allows', 'that', 'code', 'to', 'run', 'much', 'faster', 'and', 'this', 'allows', 'us', 'to', 'train', 'bigger', 'neural', 'networks', 'or', 'to', 'do', 'so', 'the', 'reason', 'or', 'multi', 'client', 'even', 'when', 'we', 'have', 'a', 'large', 'network', 'roam', 'all', 'the', 'data', 'the', 'other', 'reason', 'that', 'fast', 'computation', 'is', 'important', 'is', 'that', 'it', 'turns', 'out', 'the', 'process', 'of', 'training', 'your', 'network', 'this', 'is', 'very', 'intuitive', 'often', 'you', 'have', 'an', 'idea', 'for', 'a', 'neural', 'network', 'architecture', 'and', 'so', 'you', 'implement', 'your', 'idea', 'and', 'code', 'implementing', 'your', 'idea', 'then', 'lets', 'you', 'run', 'an', 'experiment', 'which', 'tells', 'you', 'how', 'well', 'your', 'neural', 'network', 'does', 'and', 'then', 'by', 'looking', 'at', 'it', 'you', 'go', 'back', 'to', 'change', 'the', 'details', 'of', 'your', 'new', 'network', 'and', 'then', 'you', 'go', 'around', 'this', 'circle', 'over', 'and', 'over', 'and', 'when', 'your', 'new', 'network', 'takes', 'a', 'long', 'time', 'to', 'train', 'it', 'just', 'takes', 'a', 'long', 'time', 'to', 'go', 'around', 'this', 'cycle', 'and', \"there's\", 'a', 'huge', 'difference', 'in', 'your', 'productivity', 'building', 'effective', 'neural', 'networks', 'when', 'you', 'can', 'have', 'an', 'idea', 'and', 'try', 'it', 'and', 'see', 'the', 'work', 'in', 'ten', 'minutes', 'or', 'maybe', 'ammos', 'a', 'day', 'versus', 'if', \"you've\", 'to', 'train', 'your', 'neural', 'network', 'for', 'a', 'month', 'which', 'sometimes', 'does', 'happened', 'because', 'you', 'get', 'a', 'result', 'back', 'you', 'know', 'in', 'ten', 'minutes', 'or', 'maybe', 'in', 'a', 'day', 'you', 'should', 'just', 'try', 'a', 'lot', 'more', 'ideas', 'and', 'be', 'much', 'more', 'likely', 'to', 'discover', 'in', 'your', 'network', 'and', 'it', 'works', 'well', 'for', 'your', 'application', 'and', 'so', 'faster', 'computation', 'has', 'really', 'helped', 'in', 'terms', 'of', 'speeding', 'up', 'the', 'rate', 'at', 'which', 'you', 'can', 'get', 'an', 'experimental', 'result', 'back', 'and', 'this', 'has', 'really', 'helped', 'both', 'practitioners', 'of', 'neuro', 'networks', 'as', 'well', 'as', 'researchers', 'working', 'and', 'deep', 'learning', 'iterate', 'much', 'faster', 'and', 'improve', 'your', 'ideas', 'much', 'faster', 'and', 'so', 'all', 'this', 'has', 'also', 'been', 'a', 'huge', 'boon', 'to', 'the', 'entire', 'deep', 'learning', 'research', 'community', 'which', 'has', 'been', 'incredible', 'with', 'just', 'you', 'know', 'inventing', 'new', 'algorithms', 'and', 'making', 'nonstop', 'progress', 'on', 'that', 'front', 'so', 'these', 'are', 'some', 'of', 'the', 'forces', 'powering', 'the', 'rise', 'of', 'deep', 'learning', 'but', 'the', 'good', 'news', 'is', 'that', 'these', 'forces', 'are', 'still', 'working', 'powerfully', 'to', 'make', 'deep', 'learning', 'even', 'better', 'tech', 'data', 'society', 'is', 'still', 'throwing', 'up', 'one', 'more', 'digital', 'data', 'or', 'take', 'computation', 'with', 'the', 'rise', 'of', 'specialized', 'hardware', 'like', 'gpus', 'and', 'faster', 'networking', 'many', 'types', 'of', 'hardware', \"i'm\", 'actually', 'quite', 'confident', 'that', 'our', 'ability', 'to', 'do', 'very', 'large', 'neural', 'networks', 'or', 'should', 'a', 'computation', 'point', 'of', 'view', 'will', 'keep', 'on', 'getting', 'better', 'and', 'take', 'algorithms', 'relative', 'learning', 'research', 'communities', 'though', 'continuously', 'phenomenal', 'at', 'innovating', 'on', 'the', 'algorithms', 'front', 'so', 'because', 'of', 'this', 'i', 'think', 'that', 'we', 'can', 'be', 'optimistic', 'answer', 'the', 'optimistic', 'the', 'deep', 'learning', 'will', 'keep', 'on', 'getting', 'better', 'for', 'many', 'years', 'to', 'come', 'so', 'that', \"let's\", 'go', 'on', 'to', 'the', 'last', 'video', 'of', 'the', 'section', 'where', \"we'll\", 'talk', 'a', 'little', 'bit', 'more', 'about', 'what', 'you', 'learn', 'from', 'this', 'course']\n",
            "Removing entry 143540 with contents ['being', 'effective', 'in', 'developing', 'your', 'deep', 'neural', 'nets', 'requires', 'that', 'you', 'not', 'only', 'organize', 'your', 'parameters', 'well', 'but', 'also', 'your', 'hyper', 'parameters', 'so', 'what', 'are', 'hyper', 'parameters', \"let's\", 'take', 'a', 'look', 'so', 'the', 'parameters', 'your', 'model', 'are', 'w', 'and', 'b', 'and', 'there', 'are', 'other', 'things', 'you', 'need', 'to', 'tell', 'your', 'learning', 'algorithm', 'such', 'as', 'the', 'learning', 'rate', 'alpha', 'because', 'on', 'we', 'need', 'to', 'set', 'alpha', 'and', 'that', 'in', 'turn', 'will', 'determine', 'how', 'your', 'parameters', 'evolve', 'or', 'maybe', 'the', 'number', 'of', 'iterations', 'of', 'gradient', 'descent', 'you', 'carry', 'out', 'your', 'learning', 'algorithm', 'has', 'other', 'you', 'know', 'numbers', 'that', 'you', 'need', 'to', 'set', 'such', 'as', 'the', 'number', 'of', 'hidden', 'layers', 'so', 'we', 'call', 'that', 'capital', 'l', 'or', 'the', 'number', 'of', 'hidden', 'units', 'right', 'such', 'as', 'zero', 'and', 'one', 'and', 'two', 'and', 'so', 'on', 'and', 'then', 'you', 'also', 'have', 'the', 'choice', 'of', 'activation', 'function', 'do', 'you', 'want', 'to', 'use', 'a', 'rel', 'you', 'or', 'ten', 'age', 'or', 'a', 'sigma', 'little', 'something', 'especially', 'in', 'the', 'hidden', 'layers', 'and', 'so', 'all', 'of', 'these', 'things', 'are', 'things', 'that', 'you', 'need', 'to', 'tell', 'your', 'learning', 'algorithm', 'and', 'so', 'these', 'are', 'parameters', 'that', 'control', 'the', 'ultimate', 'parameters', 'w', 'and', 'b', 'and', 'so', 'we', 'call', 'all', 'of', 'these', 'things', 'below', 'hyper', 'parameters', 'because', 'these', 'things', 'like', 'alpha', 'the', 'learning', 'rate', 'the', 'number', 'of', 'iterations', 'number', 'of', 'hidden', 'layers', 'and', 'so', 'on', 'these', 'are', 'all', 'parameters', 'that', 'control', 'w', 'and', 'b', 'so', 'we', 'call', 'these', 'things', 'hyper', 'parameters', 'because', 'it', 'is', 'the', 'hyper', 'parameters', 'that', 'you', 'know', 'somehow', 'determine', 'the', 'final', 'value', 'of', 'the', 'parameters', 'w', 'and', 'b', 'that', 'you', 'end', 'up', 'with', 'in', 'fact', 'deep', 'learning', 'has', 'a', 'lot', 'of', 'different', 'hyper', 'parameters', 'later', 'in', 'the', 'later', 'course', \"we'll\", 'see', 'other', 'hyper', 'parameters', 'as', 'well', 'such', 'as', 'the', 'momentum', 'term', 'the', 'mini', 'batch', 'size', 'various', 'forms', 'of', 'regularization', 'parameters', 'and', 'so', 'on', 'and', 'if', 'none', 'of', 'these', 'terms', 'at', 'the', 'bottom', 'make', 'sense', 'yet', \"don't\", 'worry', 'about', 'it', \"we'll\", 'talk', 'about', 'them', 'in', 'the', 'second', 'course', 'because', 'deep', 'learning', 'has', 'so', 'many', 'hyper', 'parameters', 'in', 'contrast', 'to', 'earlier', 'errors', 'of', 'machine', 'learning', \"i'm\", 'going', 'to', 'try', 'to', 'be', 'very', 'consistent', 'in', 'calling', 'the', 'learning', 'rate', 'alpha', 'a', 'hyper', 'parameter', 'rather', 'than', 'calling', 'the', 'parameter', 'i', 'think', 'in', 'earlier', 'eras', 'of', 'machine', 'learning', 'when', 'we', \"didn't\", 'have', 'so', 'many', 'hyper', 'parameters', 'most', 'of', 'us', 'used', 'to', 'be', 'a', 'bit', 'slow', 'up', 'here', 'and', 'just', 'call', 'alpha', 'a', 'parameter', 'and', 'technically', 'alpha', 'is', 'a', 'parameter', 'but', 'is', 'a', 'parameter', 'that', 'determines', 'the', 'real', 'parameters', 'our', 'childhood', 'consistent', 'in', 'calling', 'these', 'things', 'like', 'alpha', 'the', 'number', 'of', 'iterations', 'and', 'so', 'on', 'hyper', 'parameters', 'so', 'when', \"you're\", 'training', 'a', 'deep', 'net', 'for', 'your', 'own', 'application', 'you', 'find', 'that', 'there', 'may', 'be', 'a', 'lot', 'of', 'possible', 'settings', 'for', 'the', 'hyper', 'parameters', 'that', 'you', 'need', 'to', 'just', 'try', 'out', 'so', 'apply', 'deep', 'learning', 'today', 'is', 'a', 'very', 'imperiled', 'process', 'where', 'often', 'you', 'might', 'have', 'an', 'idea', 'for', 'example', 'you', 'might', 'have', 'an', 'idea', 'for', 'the', 'best', 'value', 'for', 'the', 'learning', 'rate', 'you', 'might', 'say', 'well', 'maybe', 'alpha', 'equals', '0', '01', 'i', 'want', 'to', 'try', 'that', 'then', 'you', 'implemented', 'try', 'it', 'out', 'and', 'then', 'see', 'how', 'that', 'works', 'and', 'then', 'based', 'on', 'that', 'outcome', 'you', 'might', 'say', 'you', 'know', 'what', \"i've\", 'changed', 'online', 'i', 'want', 'to', 'increase', 'the', 'learning', 'rate', 'to', '0', '05', 'and', 'so', 'if', \"you're\", 'not', 'sure', \"what's\", 'the', 'best', 'value', 'for', 'the', 'learning', 'ready', 'to', 'use', 'you', 'might', 'try', 'one', 'value', 'of', 'the', 'learning', 'rate', 'alpha', 'and', 'see', 'their', 'cost', 'function', 'j', 'go', 'down', 'like', 'this', 'then', 'you', 'might', 'try', 'a', 'larger', 'value', 'for', 'the', 'learning', 'rate', 'alpha', 'and', 'see', 'the', 'cost', 'function', 'blow', 'up', 'and', 'diverge', 'then', 'you', 'might', 'try', 'another', 'version', 'and', 'see', 'it', 'go', 'down', 'really', 'fast', \"it's\", 'inverse', 'to', 'higher', 'value', 'you', 'might', 'try', 'another', 'version', 'and', 'see', 'it', 'you', 'know', 'see', 'the', 'cost', 'function', 'j', 'do', 'that', 'then', \"i'll\", 'be', 'china', 'so', 'the', 'values', 'you', 'might', 'say', 'okay', 'looks', 'like', 'this', 'the', 'value', 'of', 'alpha', 'gives', 'me', 'a', 'pretty', 'fast', 'learning', 'and', 'allows', 'me', 'to', 'converge', 'to', 'a', 'lower', 'cost', 'function', 'jennice', \"i'm\", 'going', 'to', 'use', 'this', 'value', 'of', 'alpha', 'you', 'saw', 'in', 'a', 'previous', 'slide', 'that', 'there', 'are', 'a', 'lot', 'of', 'different', 'hybrid', 'parameters', 'and', 'it', 'turns', 'out', 'that', 'when', \"you're\", 'starting', 'on', 'the', 'new', 'application', 'i', 'should', 'find', 'it', 'very', 'difficult', 'to', 'know', 'in', 'advance', 'exactly', \"what's\", 'the', 'best', 'value', 'of', 'the', 'hyper', 'parameters', 'so', 'what', 'often', 'happen', 'is', 'you', 'just', 'have', 'to', 'try', 'out', 'many', 'different', 'values', 'and', 'go', 'around', 'this', 'cycle', 'your', 'trial', 'some', 'value', 'really', 'try', 'five', 'hidden', 'layers', 'with', 'this', 'many', 'number', 'of', 'hidden', 'units', 'implement', 'that', 'see', 'if', 'it', 'works', 'and', 'then', 'iterate', 'so', 'the', 'title', 'of', 'this', 'slide', 'is', 'that', 'apply', 'deep', 'learning', 'is', 'very', 'empirical', 'process', 'and', 'empirical', 'process', 'is', 'maybe', 'a', 'fancy', 'way', 'of', 'saying', 'you', 'just', 'have', 'to', 'try', 'a', 'lot', 'of', 'things', 'and', 'see', 'what', 'works', 'another', 'effect', \"i've\", 'seen', 'is', 'that', 'deep', 'learning', 'today', 'is', 'applied', 'to', 'so', 'many', 'problems', 'ranging', 'from', 'computer', 'vision', 'to', 'speech', 'recognition', 'to', 'natural', 'language', 'processing', 'to', 'a', 'lot', 'of', 'structured', 'data', 'applications', 'such', 'as', 'maybe', 'a', 'online', 'advertising', 'or', 'web', 'search', 'or', 'product', 'recommendations', 'and', 'so', 'on', 'and', 'what', \"i've\", 'seen', 'is', 'that', 'first', \"i've\", 'seen', 'researchers', 'from', 'one', 'discipline', 'any', 'one', 'of', 'these', 'try', 'to', 'go', 'to', 'a', 'different', 'one', 'and', 'sometimes', 'the', 'intuitions', 'about', 'hyper', 'parameters', 'carries', 'over', 'and', 'sometimes', 'it', \"doesn't\", 'so', 'i', 'often', 'advise', 'people', 'especially', 'when', 'starting', 'on', 'a', 'new', 'problem', 'to', 'just', 'try', 'out', 'a', 'range', 'of', 'values', 'and', 'see', 'what', 'works', 'and', 'then', 'mix', 'course', \"we'll\", 'see', 'a', 'systematic', 'way', \"we'll\", 'see', 'some', 'systematic', 'ways', 'for', 'trying', 'out', 'a', 'range', 'of', 'values', 'all', 'right', 'and', 'second', 'even', 'if', \"you're\", 'working', 'on', 'one', 'application', 'for', 'a', 'long', 'time', 'you', 'know', 'maybe', \"you're\", 'working', 'on', 'online', 'advertising', 'as', 'you', 'make', 'progress', 'on', 'the', 'problem', 'is', 'quite', 'possible', 'there', 'the', 'best', 'value', 'for', 'the', 'learning', 'rate', 'a', 'number', 'of', 'hidden', 'units', 'and', 'so', 'on', 'might', 'change', 'so', 'even', 'if', 'you', 'tune', 'your', 'system', 'to', 'the', 'best', 'value', 'of', 'hyper', 'parameters', 'to', 'daily', 'as', 'possible', 'you', 'find', 'that', 'the', 'best', 'value', 'might', 'change', 'a', 'year', 'from', 'now', 'maybe', 'because', 'the', 'computer', 'infrastructure', \"i'd\", 'be', 'it', 'you', 'know', 'cpus', 'or', 'the', 'type', 'of', 'gpu', 'running', 'on', 'or', 'something', 'has', 'changed', 'but', 'so', 'maybe', 'one', 'rule', 'of', 'thumb', 'is', 'you', 'know', 'every', 'now', 'and', 'then', 'maybe', 'every', 'few', 'months', 'if', \"you're\", 'working', 'on', 'a', 'problem', 'for', 'an', 'extended', 'period', 'of', 'time', 'for', 'many', 'years', 'just', 'try', 'a', 'few', 'values', 'for', 'the', 'hyper', 'parameters', 'and', 'double', 'check', 'if', \"there's\", 'a', 'better', 'value', 'for', 'the', 'hyper', 'parameters', 'and', 'as', 'you', 'do', 'so', 'you', 'slowly', 'gain', 'intuition', 'as', 'well', 'about', 'the', 'hyper', 'parameters', 'that', 'work', 'best', 'for', 'your', 'problems', 'and', 'i', 'know', 'that', 'this', 'might', 'seem', 'like', 'an', 'unsatisfying', 'part', 'of', 'deep', 'learning', 'that', 'you', 'just', 'have', 'to', 'try', 'on', 'all', 'the', 'values', 'for', 'these', 'hyper', 'parameters', 'but', 'maybe', 'this', 'is', 'one', 'area', 'where', 'deep', 'learning', 'research', 'is', 'still', 'advancing', 'and', 'maybe', 'over', 'time', \"we'll\", 'be', 'able', 'to', 'give', 'better', 'guidance', 'for', 'the', 'best', 'hyper', 'parameters', 'to', 'use', 'but', \"it's\", 'also', 'possible', 'that', 'because', 'cpus', 'and', 'gpus', 'and', 'networks', 'and', 'data', 'says', 'are', 'all', 'changing', 'and', 'it', 'is', 'possible', 'that', 'the', 'guidance', \"won't\", 'to', 'converge', 'for', 'some', 'time', 'and', 'you', 'just', 'need', 'to', 'keep', 'trying', 'out', 'different', 'values', 'and', 'evaluate', 'them', 'on', 'a', 'hold', 'on', 'cross', 'validation', 'set', 'or', 'something', 'and', 'pick', 'the', 'value', 'that', 'works', 'for', 'your', 'problems', 'so', 'that', 'was', 'a', 'brief', 'discussion', 'of', 'hyper', 'parameters', 'in', 'the', 'second', 'course', \"we'll\", 'also', 'give', 'some', 'suggestions', 'for', 'how', 'to', 'systematically', 'explore', 'the', 'space', 'of', 'hyper', 'parameters', 'but', 'by', 'now', 'you', 'actually', 'have', 'pretty', 'much', 'all', 'the', 'tools', 'you', 'need', 'to', 'do', 'their', 'programming', 'exercise', 'before', 'you', 'do', 'that', 'adjust', 'or', 'share', 'view', 'one', 'more', 'set', 'of', 'ideas', 'which', 'is', 'i', 'often', 'ask', 'what', 'does', 'deep', 'learning', 'have', 'to', 'do', 'the', 'human', 'brain']\n",
            "Removing entry 147683 with contents ['music', 'this', 'video', 'shows', 'you', 'how', 'to', 'create', 'a', 'jupiter', 'notebook', \"let's\", 'start', 'by', 'adding', 'a', 'data', 'asset', 'to', 'the', 'project', 'you', 'can', 'either', 'browse', 'to', 'select', 'files', 'or', 'drag', 'files', 'into', 'the', 'panel', 'great', 'now', 'the', 'data', 'file', 'is', 'uploaded', 'to', 'object', 'storage', 'and', 'available', 'as', 'a', 'data', 'asset', 'in', 'this', 'project', 'next', 'create', 'a', 'notebook', 'provide', 'a', 'name', 'and', 'a', 'description', 'and', 'then', 'select', 'the', 'runtime', 'to', 'use', 'when', 'running', 'this', 'notebook', 'here', 'you', 'see', 'the', 'environments', 'you', 'could', 'use', \"you'll\", 'learn', 'more', 'about', 'environments', 'later', 'so', 'for', 'now', 'just', 'select', 'the', 'default', 'spark', 'python', 'environment', 'and', 'verify', 'the', 'language', 'and', 'spark', 'version', 'when', 'you', 'ready', 'create', 'the', 'notebook', 'now', 'wait', 'while', 'the', 'runtime', 'environment', 'is', 'instantiated', 'once', 'the', 'environment', 'is', 'ready', 'in', 'the', 'notebook', 'access', 'the', 'data', 'sources', 'and', 'locate', 'the', 'file', 'click', 'insert', 'to', 'code', 'and', 'choose', 'how', 'you', 'want', 'to', 'insert', 'the', 'data', 'the', 'choices', 'in', 'this', 'drop', 'down', 'box', 'are', 'dependent', 'upon', 'the', 'language', 'used', 'in', 'this', 'notebook', 'and', 'the', 'file', 'type', 'notice', 'that', 'the', 'inserted', 'code', 'includes', 'the', 'credentials', \"you'll\", 'need', 'to', 'read', 'the', 'data', 'file', 'from', 'the', 'object', 'storage', 'instance', 'when', 'you', 'run', 'the', 'code', 'the', 'first', 'five', 'rows', 'display', 'now', \"let's\", 'take', 'a', 'closer', 'look', 'at', 'environments', 'on', 'the', 'environments', 'tab', 'you', 'can', 'define', 'the', 'hardware', 'size', 'and', 'software', 'configuration', 'for', 'the', 'runtime', 'associated', 'with', 'watson', 'studio', 'tools', 'such', 'as', 'notebooks', 'you', 'can', 'see', 'that', 'there', 'is', 'one', 'active', 'environment', 'runtime', 'namely', 'the', 'runtime', 'being', 'used', 'by', 'the', 'notebook', 'you', 'just', 'created', 'and', 'here', 'are', 'the', 'other', 'default', 'environments', 'you', 'can', 'view', 'any', 'of', 'the', 'default', 'environments', 'to', 'see', 'a', 'summary', 'of', 'the', 'configuration', 'and', 'also', 'create', 'a', 'new', 'environment', 'definition', 'first', 'provide', 'a', 'name', 'in', 'a', 'description', 'if', 'you', 'select', 'spark', 'for', 'the', 'type', \"you'll\", 'see', 'some', 'additional', 'configuration', 'options', 'in', 'this', 'case', 'just', 'accept', 'the', 'defaults', 'and', 'choose', 'scala', 'for', 'the', 'software', 'version', 'when', 'you', 'ready', 'create', 'the', 'new', 'environment', 'the', 'environment', 'is', 'ready', 'for', 'you', 'to', 'use', 'with', 'a', 'notebook', 'to', 'switch', 'a', 'notebook', 'to', 'use', 'a', 'different', 'environment', 'you', 'need', 'to', 'first', 'stop', 'the', 'colonel', 'then', 'you', 'can', 'change', 'the', 'environment', 'and', 'select', 'the', 'custom', 'environment', 'you', 'just', 'created', 'and', 'associate', 'that', 'with', 'the', 'notebook', 'now', 'open', 'the', 'notebook', 'in', 'edit', 'mode', 'and', 'wait', 'for', 'the', 'new', 'environment', 'to', 'be', 'instantiated', 'since', 'this', 'notebook', 'was', 'last', 'saved', 'using', 'a', 'different', 'kernel', 'you', 'need', 'to', 'set', 'the', 'new', 'kernel', \"let's\", 'delete', 'the', 'existing', 'cell', 'locate', 'the', 'source', 'data', 'file', 'and', 'insert', 'a', 'spark', 'session', 'data', 'frame', 'when', 'you', 'run', 'the', 'code', 'the', 'first', 'five', 'rows', 'display', 'now', \"you're\", 'ready', 'to', 'explore', 'the', 'community', 'and', 'find', 'sample', 'notebooks', 'and', 'datasets', 'to', 'get', 'started', 'analyzing', 'data', 'music']\n",
            "Removing entry 147700 with contents ['music', 'this', 'video', 'shows', 'you', 'how', 'to', 'connect', 'your', 'ibm', 'watson', 'studio', 'account', 'with', 'your', 'github', 'account', 'in', 'watson', 'studio', 'navigate', 'to', 'your', 'profile', 'settings', 'on', 'the', 'integrations', 'tab', 'visit', 'the', 'link', 'to', 'generate', 'a', 'github', 'personal', 'access', 'token', 'provide', 'a', 'descriptive', 'name', 'for', 'the', 'token', 'and', 'select', 'the', 'repo', 'scope', 'then', 'generate', 'the', 'token', 'copy', 'the', 'token', 'return', 'to', 'the', 'github', 'integration', 'settings', 'and', 'paste', 'the', 'token', 'the', 'token', 'is', 'validated', 'when', 'you', 'save', 'it', 'to', 'your', 'profile', 'settings', 'now', 'navigate', 'to', 'your', 'projects', 'you', 'enable', 'github', 'integration', 'at', 'the', 'project', 'level', 'on', 'the', 'settings', 'tab', 'simply', 'scroll', 'to', 'the', 'bottom', 'and', 'paste', 'the', 'existing', 'github', 'repository', 'url', 'once', 'the', 'url', 'is', 'validated', 'click', 'connect', 'go', 'to', 'the', 'assets', 'tab', 'and', 'open', 'the', 'notebook', 'you', 'want', 'to', 'publish', 'notice', 'that', 'this', 'notebook', 'has', 'the', 'credentials', 'replaced', 'with', \"x's\", \"it's\", 'a', 'best', 'practice', 'to', 'remove', 'or', 'replace', 'credentials', 'before', 'publishing', 'to', 'github', 'so', 'this', 'notebook', 'is', 'ready', 'for', 'publishing', 'you', 'can', 'provide', 'the', 'target', 'path', 'along', 'with', 'a', 'commit', 'message', 'you', 'also', 'have', 'the', 'option', 'to', 'publish', 'content', 'without', 'hidden', 'code', 'which', 'means', 'that', 'any', 'cells', 'in', 'the', 'notebook', 'that', 'began', 'with', 'the', 'hidden', 'cell', 'comment', 'will', 'not', 'be', 'published', 'when', \"you're\", 'ready', 'click', 'publish', 'the', 'message', 'tells', 'you', 'that', 'the', 'notebook', 'was', 'published', 'successfully', 'and', 'provides', 'links', 'to', 'the', 'notebook', 'the', 'repository', 'in', 'the', 'commit', \"let's\", 'take', 'a', 'look', 'at', 'the', 'commits', 'so', \"there's\", 'the', 'commit', 'and', 'you', 'can', 'navigate', 'to', 'the', 'repository', 'to', 'see', 'the', 'publish', 'notebook', 'lastly', 'you', 'can', 'publish', 'as', 'a', 'gist', 'just', 'are', 'another', 'way', 'to', 'share', 'your', 'work', 'on', 'github', 'every', 'gist', 'is', 'a', 'git', 'repository', 'so', 'it', 'can', 'be', 'forked', 'and', 'cloned', 'there', 'are', 'two', 'types', 'of', 'gists', 'public', 'and', 'secret', 'if', 'you', 'start', 'out', 'with', 'a', 'secret', 'gist', 'you', 'can', 'convert', 'it', 'to', 'a', 'public', 'gist', 'later', 'and', 'again', 'you', 'have', 'the', 'option', 'to', 'remove', 'hidden', 'cells', 'so', \"that's\", 'the', 'basics', 'of', 'watson', 'studios', 'github', 'integration', 'music']\n",
            "Removing entry 147701 with contents ['watson', 'studio', 'is', 'an', 'integrated', 'platform', 'of', 'tools', 'services', 'and', 'data', 'that', 'helps', 'companies', 'accelerate', 'their', 'shift', 'to', 'become', 'data', 'driven', 'organizations', 'you', 'can', 'start', 'with', 'a', 'free', 'account', 'to', 'explore', 'its', 'capabilities', 'data', 'science', 'is', 'a', 'team', 'sport', 'we', 'have', 'different', 'types', 'of', 'people', 'interested', 'in', 'the', 'insights', 'that', 'data', 'science', 'can', 'provide', 'this', 'includes', 'business', 'analysts', 'data', 'engineers', 'data', 'stewards', 'data', 'scientist', 'and', 'developers', 'data', 'needs', 'to', 'be', 'located', 'in', 'cleanse', 'models', 'have', 'to', 'be', 'created', 'tested', 'monitored', 'and', 'updated', 'all', 'this', 'requires', 'teamwork', 'for', 'this', 'reason', 'watson', 'city', 'was', 'built', 'as', 'a', 'collaborative', 'platform', 'a', 'community', 'of', 'like', 'minded', 'people', 'there', 'is', 'a', 'lot', 'to', 'cover', 'in', 'this', 'introduction', 'and', 'will', 'only', 'scratch', 'the', 'surface', 'you', 'can', 'find', 'more', 'information', 'on', 'the', 'digital', 'technical', 'engagement', 'site', 'at', 'ibm', 'com', 'slash', 'demos', 'once', 'you', 'are', 'logged', 'in', 'you', 'may', 'see', 'the', 'get', 'started', 'welcome', 'screen', 'you', 'can', 'minimize', 'the', 'screen', 'by', 'clicking', 'on', 'the', 'get', 'started', 'button', 'in', 'the', 'upper', 'right', 'one', 'important', 'item', 'that', 'is', 'easy', 'to', 'miss', 'is', 'the', 'hamburger', 'button', 'in', 'the', 'upper', 'left', 'it', 'gives', 'you', 'direct', 'access', 'to', 'projects', 'catalogs', 'and', 'services', 'among', 'other', 'things', 'the', 'gallery', 'is', 'particularly', 'interesting', 'it', 'is', 'a', 'collection', 'of', 'assets', 'including', 'tutorials', 'notebooks', 'datasets', 'articles', 'and', 'papers', 'from', 'multiple', 'sources', 'new', 'assets', 'are', 'constantly', 'added', 'assets', 'can', 'be', 'searched', 'using', 'filters', 'for', 'thai', 'language', 'technology', 'topics', 'and', 'so', 'on', 'the', 'results', 'can', 'be', 'sorted', 'by', 'features', 'or', 'by', 'date', 'manage', 'gives', 'you', 'quick', 'access', 'to', 'specific', 'areas', 'to', 'manage', 'finally', 'we', 'have', 'integrated', 'support', 'and', 'documentation', 'in', 'the', 'watson', 'environment', 'as', 'mentioned', 'earlier', 'the', 'project', 'is', 'the', 'center', 'of', 'the', 'collaboration', 'it', 'is', 'very', 'simple', 'to', 'create', 'a', 'project', 'you', 'click', 'create', 'a', 'project', 'in', 'the', 'welcome', 'screen', 'or', 'new', 'project', 'and', 'either', 'create', 'an', 'empty', 'project', 'or', 'one', 'from', 'an', 'existing', 'one', 'then', 'you', 'give', 'it', 'a', 'name', 'possibly', 'add', 'a', 'description', 'and', \"you're\", 'ready', 'to', 'go', 'at', 'the', 'project', 'level', 'we', 'also', 'have', 'a', 'menu', 'of', 'options', 'it', 'starts', 'with', 'the', 'overview', 'where', 'you', 'can', 'see', 'basic', 'information', 'on', 'the', 'project', 'this', 'tab', 'also', 'includes', 'a', 'readme', 'section', 'where', 'you', 'can', 'get', 'more', 'details', 'on', 'what', 'the', 'project', 'is', 'about', 'the', 'next', 'one', 'is', 'assets', 'where', 'you', 'can', 'see', 'the', 'data', 'assets', 'models', 'notebooks', 'and', 'other', 'assets', 'that', 'are', 'part', 'of', 'the', 'project', 'you', 'can', 'go', 'to', 'add', 'specific', 'assets', 'using', 'the', 'add', 'to', 'project', 'drop', 'down', 'menu', 'at', 'the', 'top', 'of', 'the', 'screen', 'we', \"won't\", 'go', 'into', 'all', 'of', 'those', 'menu', 'items', 'but', 'one', 'important', 'want', 'to', 'know', 'is', 'connection', 'this', 'allows', 'you', 'to', 'access', 'data', 'that', 'comes', 'from', 'outside', 'watson', 'studio', 'as', 'you', 'can', 'see', 'it', 'includes', 'a', 'lot', 'of', 'data', 'services', 'from', 'ibm', 'but', 'also', 'quite', 'a', 'few', 'from', 'third', 'parties', 'such', 'as', 'amazon', 'and', 'microsoft', 'going', 'back', 'to', 'our', 'project', \"i'd\", 'like', 'to', 'point', 'out', 'the', 'environment', 'section', 'one', 'important', 'tool', 'for', 'that', 'exploration', 'data', 'manipulation', 'and', 'model', 'creation', 'is', 'the', 'notebook', 'depending', 'on', 'the', 'amount', 'of', 'work', 'that', 'needs', 'to', 'be', 'done', 'we', 'have', 'a', 'choice', 'of', 'resource', 'allocation', 'we', 'can', 'also', 'tailor', 'the', 'environment', 'to', 'include', 'additional', 'libraries', 'so', 'we', 'have', 'a', 'complete', 'environment', 'from', 'the', 'start', 'i', 'want', 'to', 'point', 'out', 'two', 'more', 'selections', 'from', 'the', 'top', 'menu', 'access', 'control', 'and', 'settings', 'the', 'access', 'control', 'allows', 'you', 'to', 'control', 'collaborators', 'and', 'their', 'permissions', 'and', 'more', 'in', 'the', 'settings', 'section', 'you', 'can', 'among', 'other', 'things', 'add', 'services', 'for', 'example', 'you', 'click', 'on', 'the', 'add', 'service', 'drop', 'down', 'menu', 'select', 'watson', 'and', 'add', 'a', 'machine', 'learning', 'service', 'you', 'have', 'the', 'choice', 'to', 'add', 'an', 'existing', 'service', 'you', 'may', 'have', 'created', 'earlier', 'in', 'another', 'project', 'or', 'create', 'a', 'new', 'one', 'note', 'that', 'most', 'services', 'include', 'a', 'light', 'free', 'version', 'this', 'means', 'that', 'you', 'can', 'experiment', 'with', 'all', 'sorts', 'of', 'capabilities', 'for', 'free', 'you']\n",
            "Removing entry 188435 with contents ['hello', 'and', 'welcome', 'to', 'working', 'with', 'right', 'outer', 'join', 'in', 'this', 'video', 'we', 'will', 'learn', 'about', 'creating', 'a', 'result', 'set', 'by', 'joining', 'rows', 'from', 'two', 'tables', 'at', 'the', 'end', 'of', 'this', 'lesson', 'you', 'will', 'be', 'able', 'to', 'explain', 'the', 'syntax', 'of', 'the', 'right', 'outer', 'join', 'operator', 'and', 'interpret', 'the', 'result', 'set', 'a', 'join', 'combines', 'the', 'rows', 'from', 'two', 'or', 'more', 'tables', 'based', 'on', 'a', 'relationship', 'between', 'certain', 'columns', 'in', 'these', 'tables', 'there', 'are', 'two', 'types', 'of', 'table', 'joins', 'inner', 'join', 'and', 'outer', 'join', 'an', 'outer', 'join', 'is', 'a', 'specialized', 'form', 'of', 'join', 'and', 'there', 'are', 'three', 'types', 'of', 'outer', 'join', 'left', 'outer', 'join', 'right', 'outer', 'join', 'and', 'full', 'outer', 'join', 'this', 'video', 'explains', 'right', 'outer', 'join', 'or', 'simply', 'called', 'right', 'join', 'looking', 'at', 'the', 'diagram', 'the', 'terms', 'left', 'and', 'right', 'refer', 'to', 'the', 'table', 'on', 'the', 'left', 'hand', 'side', 'and', 'the', 'right', 'hand', 'side', 'of', 'the', 'diagram', 'in', 'this', 'diagram', 'table', 'one', 'is', 'the', 'left', 'table', 'a', 'right', 'join', 'matches', 'the', 'results', 'from', 'two', 'tables', 'and', 'displays', 'all', 'the', 'rows', 'from', 'the', 'right', 'table', 'and', 'combines', 'the', 'information', 'with', 'rows', 'from', 'the', 'left', 'table', 'that', 'match', 'the', 'criteria', 'specified', 'in', 'the', 'query', 'in', 'this', 'diagram', 'the', 'result', 'set', 'of', 'a', 'right', 'join', 'is', 'all', 'rows', 'from', 'both', 'tables', 'matching', 'the', 'criteria', 'specified', 'in', 'the', 'query', 'plus', 'all', 'non', 'matching', 'rows', 'from', 'the', 'right', 'table', 'based', 'on', 'our', 'simplified', 'library', 'database', 'model', 'if', 'we', 'want', 'to', 'check', 'the', 'status', 'of', 'all', 'books', 'out', 'on', 'loan', 'this', 'information', 'is', 'split', 'between', 'two', 'tables', 'the', 'loan', 'table', 'which', 'includes', 'the', \"borrower's\", 'id', 'and', 'loan', 'date', 'and', 'the', 'borrower', 'table', 'which', 'includes', 'the', \"borrower's\", 'id', 'first', 'name', 'and', 'last', 'name', 'in', 'an', 'outer', 'join', 'the', 'first', 'table', 'specified', 'in', 'the', 'from', 'clause', 'of', 'the', 'sequel', 'statement', 'is', 'referred', 'to', 'as', 'the', 'left', 'table', 'and', 'the', 'remaining', 'table', 'is', 'referred', 'to', 'as', 'the', 'right', 'table']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imuTcZ6vvyM_",
        "outputId": "4b159740-8a2d-497e-8484-81dce145c3bb"
      },
      "source": [
        "# Checking how man sentence lengths greater than 50\n",
        "large = [i for i in sent_lengths if i>50]\n",
        "\n",
        "# We see that most of the lengths are rather small so we will pad sequences to 100 and keep 100 tokens as max sequence length \n",
        "\n",
        "precentage_loss= len(large)/len(sent_lengths)\n",
        "print(precentage_loss)"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.01366009414736064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecbI-9T1rpyQ"
      },
      "source": [
        "## Forming Document Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO8RH98pquwj"
      },
      "source": [
        "# Forming a Dictionary where each entry in the dict is a list of lists that consists of the words in each sentence for all sentences in the transcript\n",
        "i = 0 \n",
        "doc_dict = {}\n",
        "for test_seq in all_sentences:\n",
        "  i +=1\n",
        "  doc_seq = tokenizer.texts_to_sequences(test_seq)\n",
        "  padded_seq = pad_sequences(doc_seq, maxlen=50, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
        "  doc_sent_words = []\n",
        "  for j in padded_seq:\n",
        "    #print(j) \n",
        "    sent_words = [inv_word_index[i] for i in j]\n",
        "    doc_sent_words.append(sent_words)\n",
        "    doc_dict[i]= doc_sent_words"
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oos61g6yqvNH",
        "outputId": "8e87860d-e51f-419c-d07a-c885dffdcb50"
      },
      "source": [
        "doc_dict[79][1]"
      ],
      "execution_count": 354,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ai',\n",
              " 'is',\n",
              " 'changing',\n",
              " 'the',\n",
              " 'way',\n",
              " 'we',\n",
              " 'work',\n",
              " 'and',\n",
              " 'live',\n",
              " 'and',\n",
              " 'this',\n",
              " 'nontechnical',\n",
              " 'course',\n",
              " 'will',\n",
              " 'teach',\n",
              " 'you',\n",
              " 'how',\n",
              " 'to',\n",
              " 'navigate',\n",
              " 'the',\n",
              " 'rise',\n",
              " 'of',\n",
              " 'ai',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 354
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RM_0AFRvQaG"
      },
      "source": [
        "# Downloading the GloVe Embeddings "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "za6N7Ovv_5jO",
        "outputId": "7032c387-4273-43a2-eeb0-f19e2126c37e"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-22 01:57:57--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-12-22 01:57:57--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-12-22 01:57:58--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.06MB/s    in 6m 30s  \n",
            "\n",
            "2020-12-22 02:04:28 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAKh33SdSViN"
      },
      "source": [
        "## Forming the Glove Embeddings Index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxZsO-T1_5lj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b756bc9-588d-4733-8d41-2758d039a6ce"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "import numpy as np\n",
        "\n",
        "path_to_glove_file = \"/content/glove.6B.200d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": 333,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKNBqsNz_5n2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "240c50b8-bb2c-4504-f1c3-af27bc2b039a"
      },
      "source": [
        "# \n",
        "num_tokens = len(word_index) + 2\n",
        "embedding_dim = 200\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converted 22437 words (5449 misses)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xzwiEUlSJ--"
      },
      "source": [
        "# Now we will go document by document and form the average document embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siaSma7D_5xQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01cc2664-767a-4941-cfca-fa929a8e3dcc"
      },
      "source": [
        "doc_matrix_dict = {}\n",
        "for doc_id, doc in doc_dict.items():\n",
        "  sent_matrix = np.empty((0,200), float)\n",
        "  sentence_count = 0\n",
        "  doc_matrix_dict[doc_id] ={}  \n",
        "  #print(doc_id, doc_matrix_dict[doc_id])\n",
        "  for sent in doc:\n",
        "    sentence_count += 1\n",
        "    embedding_matrix = []\n",
        "    for word in sent:\n",
        "        word_vect = embeddings_index.get(word)\n",
        "        if word_vect is not None:\n",
        "          embedding_matrix.append(word_vect)                                           # Get the word embedding for each word \n",
        "        \n",
        "    embedding_array = np.array(embedding_matrix)                                       # Store all the word embedding in a sentence in an array\n",
        "    sent_embedding = np.mean(embedding_array, axis = 0 )                               # Get the sentence embedding as average of all word embedding in the sentence \n",
        "    try:\n",
        "      sent_matrix = np.append(sent_matrix, np.array([sent_embedding]),axis = 0)        # Store all the sentence embedding in sent_matrix                           \n",
        "    except:\n",
        "      print(\"Skipping concatenation at doc\",doc_id,\"sentence\", sentence_count,\"due to [nan]/missing value error\") \n",
        "\n",
        "    doc_matrix_dict[doc_id][sentence_count] = sent_embedding                           # Store the sentence embedding of each sentence for a document in as an item in a dictionary\n",
        "\n",
        "  doc_embedding = np.mean(sent_matrix, axis = 0)                                       # Compute doc embedding as mean of all the sentence embeddings \n",
        "  doc_matrix_dict[doc_id][\"document_embedding\"]= doc_embedding                         # Store the overall document embedding as well "
      ],
      "execution_count": 335,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping concatenation at doc 206 sentence 92 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 255 sentence 98 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 255 sentence 107 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 528 sentence 44 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 658 sentence 225 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 666 sentence 4 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 888 sentence 14 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 913 sentence 28 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 913 sentence 149 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 914 sentence 79 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 1066 sentence 75 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 1084 sentence 65 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 1089 sentence 82 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 1089 sentence 98 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 1103 sentence 80 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 1106 sentence 62 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 1119 sentence 28 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 1119 sentence 34 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 1119 sentence 44 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 1119 sentence 138 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 1159 sentence 101 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 1562 sentence 164 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 1573 sentence 125 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 2859 sentence 2 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 3060 sentence 77 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 3160 sentence 77 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 3185 sentence 13 due to [nan] error aka missing value\n",
            "Skipping concatenation at doc 3263 sentence 13 due to [nan] error aka missing value\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs_rxR1z_5zn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a274b329-775b-4c7c-f9b1-e6bce5eb029b"
      },
      "source": [
        "# We now have the sentence embeddings as well as the document embedding in this dictionary \n",
        "\n",
        "print(\"Embedding for Overall Document 80 \")\n",
        "print(doc_matrix_dict[80][\"document_embedding\"])\n",
        "\n",
        "# Embedding for sentence 12\n",
        "print(\"\\n\\nEmbedding for Sentence 1 in Document 80 \")\n",
        "print(doc_matrix_dict[80][1])"
      ],
      "execution_count": 362,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding for Overall Document 80 \n",
            "[ 2.50772094e-01  2.07434424e-01 -2.39391313e-02 -1.85828246e-01\n",
            "  1.05222334e-01  5.16955882e-02 -4.18029620e-01  8.96055067e-02\n",
            "  5.30375720e-02  1.24387031e-01 -2.27594664e-02  2.69181277e-01\n",
            "  1.43676549e-01  4.87895079e-02  1.91051073e-01  9.46232169e-02\n",
            " -1.02217908e-01  3.07664605e-01 -3.37000431e-02 -1.67255046e-01\n",
            "  1.48605987e-01  2.62409929e+00 -7.22534710e-02 -8.03505262e-02\n",
            "  1.75544009e-01 -5.67431986e-02 -9.00074481e-02  5.52811609e-02\n",
            "  4.43384706e-02  2.94253222e-03 -6.15333321e-02 -7.43854059e-02\n",
            " -1.02617347e-02 -6.76434284e-03 -8.66740307e-04 -3.08234126e-01\n",
            " -5.25914338e-01 -3.25696104e-01 -1.21299262e-02  3.92275077e-02\n",
            " -1.10708819e-02 -1.14774081e-01  1.17031462e-02  3.10713242e-01\n",
            " -1.35455775e-01  1.55831467e-01  4.60764528e-01 -1.17356452e-01\n",
            "  3.14563344e-02  1.91563342e-01 -5.36601985e-02 -5.68585401e-02\n",
            " -8.91179594e-02  3.39624549e-01  1.93689677e-01 -2.27715848e-02\n",
            " -7.64871847e-02 -5.65174749e-02 -2.96261575e-02  9.39446585e-02\n",
            "  7.20797482e-02  6.69381120e-02 -2.29862612e-01 -4.90606626e-02\n",
            "  6.92502711e-02 -6.51986794e-03 -5.92358170e-02  2.45287537e-01\n",
            "  5.17456723e-02  5.86482534e-02  3.72324775e-01  8.68673215e-02\n",
            "  5.04746290e-02  1.19637134e-01 -1.68916361e-01  2.25024484e-01\n",
            " -2.40334700e-01 -1.69405092e-01 -2.04324031e-01 -1.00672570e-01\n",
            " -2.09701122e-02 -1.37971467e-02 -1.27490230e-01  1.12785321e-01\n",
            "  1.14996069e-01 -5.62666012e-02 -2.30482677e-01 -2.76092582e-01\n",
            "  5.81906188e-01 -6.87280650e-01  2.15047409e-01  4.34308306e-02\n",
            "  3.12324105e-01  1.14479468e-01 -7.65006891e-02  8.64683766e-02\n",
            "  9.61878046e-02 -9.14958235e-02 -7.22349005e-02 -9.04506308e-02\n",
            "  1.48093173e-01  1.25595012e-02 -8.52660065e-02  1.22137870e-01\n",
            "  8.42081965e-02 -2.39642210e-02 -1.79847759e-02  9.80599974e-01\n",
            " -3.41839569e-01  2.44464259e-02  9.19798842e-02 -1.59278901e-01\n",
            "  1.11702596e-01  2.89745872e-02 -2.39849443e-02  2.46870451e-02\n",
            "  4.81167105e-02 -7.54042502e-03 -2.68093633e-01 -1.78265749e-01\n",
            "  3.08618495e-01  7.44374678e-02  2.06402341e-01 -7.33673710e-02\n",
            " -6.19756122e-02 -3.33820720e-01  2.07796154e-01  1.68831736e-01\n",
            "  1.20618855e-01 -8.22452330e-02 -3.32499202e-01  1.49832267e-01\n",
            "  1.57730085e-01 -1.82171860e-01  9.47776460e-02  1.29327883e-01\n",
            " -1.44661021e-02 -1.74750260e-01 -5.58716937e-02 -8.43246237e-02\n",
            " -1.54223088e-02 -3.78633824e-04  9.83010802e-02 -1.75284955e-01\n",
            "  1.17121983e+00  1.64482962e-01 -4.51277730e-02 -3.48244103e-01\n",
            "  9.67136089e-02  5.38912441e-03  1.28991258e-01  2.26997405e-01\n",
            " -2.05408976e-01 -2.87709056e-02  1.66782758e-01 -9.71920352e-02\n",
            " -1.28670065e-01  1.12890663e-01  1.07565810e-02 -2.54108462e-01\n",
            "  2.21031731e-02 -1.68648848e-02 -2.36196351e-02 -7.14459543e-02\n",
            " -2.80999556e-02  7.25231206e-02 -6.67435616e-02  8.70313017e-02\n",
            " -3.06355122e-01  3.75860000e-01 -3.78568694e-02  2.56913798e-02\n",
            "  2.88757820e-01  1.72842846e-02  9.42447068e-02  4.92540471e-02\n",
            " -1.92971734e-01 -9.38375825e-02  3.91344188e-02  1.27203365e-01\n",
            "  1.04560103e+00 -1.93962776e-01 -1.23107358e-01  5.07236886e-02\n",
            " -5.39249314e-02 -1.34798171e-01  3.74823583e-02  4.39253673e-02\n",
            "  5.38867973e-02  1.47265213e-01 -2.09697282e-01  2.71846164e-02\n",
            " -7.94138506e-03  1.09678379e-01  1.09589759e-01 -7.44402564e-02\n",
            " -2.22799655e-02  1.57570537e-01 -1.03106056e-01  7.67451795e-02]\n",
            "\n",
            "\n",
            "Embedding for Sentence 1 in Document 80 \n",
            "[ 2.57135421e-01  2.35011548e-01 -8.51764111e-04 -1.68136746e-01\n",
            "  8.00535157e-02  2.57650092e-02 -5.03407419e-01  8.81903321e-02\n",
            "  1.25766769e-01  1.80340916e-01 -5.40834665e-02  2.39488393e-01\n",
            "  2.02564210e-01  7.42789805e-02  2.12914765e-01  3.72915894e-01\n",
            " -1.91036463e-01  3.89293045e-01  6.56249374e-02 -1.37029082e-01\n",
            "  7.03742355e-02  2.68450069e+00 -1.45809755e-01 -1.91636190e-01\n",
            "  1.99846536e-01  3.11437948e-03 -7.74907693e-02 -1.45310618e-03\n",
            "  3.05083357e-02  1.99025385e-02 -1.59190685e-01 -3.43585350e-02\n",
            " -1.13116615e-01  8.30678549e-03  3.70388888e-02 -3.27788711e-01\n",
            " -5.14044523e-01 -3.01959068e-01 -6.12579137e-02  1.47266686e-01\n",
            " -1.19427003e-01 -1.30641460e-01 -2.04231918e-01  3.17228347e-01\n",
            " -1.72036007e-01  1.80990219e-01  4.54746127e-01 -3.10197145e-01\n",
            "  1.02263186e-02  1.87022939e-01 -2.95687988e-02 -4.92093191e-02\n",
            " -8.02243650e-02  3.31166387e-01  5.15820831e-02 -9.85252336e-02\n",
            "  2.35319138e-02 -4.92665358e-02 -1.37057930e-01  1.61768988e-01\n",
            "  8.86119232e-02  1.72214508e-01 -1.28028527e-01  7.07198307e-02\n",
            " -9.62768623e-04 -1.35253251e-01 -1.62857294e-01  3.02340806e-01\n",
            "  5.12636080e-02  1.27075627e-01  3.80436450e-01  8.02124217e-02\n",
            "  7.96705410e-02  7.09839910e-02 -2.46301010e-01  1.47361994e-01\n",
            " -2.65256435e-01 -1.58021316e-01 -2.22677916e-01 -1.56903997e-01\n",
            " -2.75213588e-02  6.23500124e-02 -1.26886159e-01  2.29389116e-01\n",
            "  5.24450876e-02 -1.25894532e-01 -2.56578684e-01 -1.23255089e-01\n",
            "  7.13171005e-01 -8.10984671e-01  3.15524250e-01  4.63817716e-02\n",
            "  3.64981532e-01 -2.27135438e-02 -2.29622990e-01  1.69300154e-01\n",
            "  1.93998992e-01 -9.69975367e-02 -4.36257757e-02 -1.31354317e-01\n",
            "  2.76615232e-01 -1.61910933e-02 -2.16494612e-02  1.73338011e-01\n",
            "  1.56685457e-01  3.20001133e-02 -3.64072286e-02  9.02102411e-01\n",
            " -3.90988529e-01  1.45434156e-01  1.13448329e-01 -2.31917307e-01\n",
            "  9.17886943e-02  8.56586844e-02 -9.05165523e-02  4.52083088e-02\n",
            " -1.27643850e-02 -1.56407673e-02 -2.67702371e-01 -5.48790842e-02\n",
            "  4.20428485e-01  9.82652456e-02  3.38976175e-01 -3.15277837e-02\n",
            " -1.32460579e-01 -2.90235400e-01  9.41349193e-02  1.13512471e-01\n",
            "  1.50232375e-01 -1.47753388e-01 -4.78137642e-01  3.02290529e-01\n",
            "  1.95094377e-01 -1.67489603e-01  7.22092316e-02  7.58504942e-02\n",
            " -2.50740573e-02  4.51375395e-02  1.77176166e-02 -2.02783942e-03\n",
            " -1.07147098e-01 -2.41999999e-02  5.06479926e-02 -3.47537011e-01\n",
            "  1.18109775e+00  2.47441530e-01 -9.46179256e-02 -3.68840754e-01\n",
            "  1.21493466e-01 -6.53192550e-02  1.39106929e-01  2.23870218e-01\n",
            " -2.44383156e-01  1.73155759e-02  2.35925108e-01 -2.02110499e-01\n",
            " -1.01655357e-01  1.05790392e-01  8.81411731e-02 -2.47696847e-01\n",
            "  7.29800314e-02  2.23219208e-02 -1.75831690e-01 -1.00586288e-01\n",
            "  4.79132012e-02  1.55179247e-01  7.22288853e-03  1.39934003e-01\n",
            " -1.83086023e-01  4.08696920e-01 -4.77312207e-02  2.42018569e-02\n",
            "  3.11825126e-01 -5.18558435e-02 -4.12154868e-02 -4.76367474e-02\n",
            " -1.00313991e-01 -2.92157698e-02  5.25156185e-02  1.70694605e-01\n",
            "  1.03313613e+00 -1.30132228e-01 -1.65481076e-01 -2.56133098e-02\n",
            "  8.81245546e-03 -8.19290206e-02  1.37619540e-01  1.53722733e-01\n",
            "  1.10305384e-01  1.76340982e-01 -2.49105617e-01  4.06336188e-02\n",
            "  1.06325336e-02  1.22277379e-01  4.71325330e-02 -1.11832768e-01\n",
            "  1.88482292e-02  1.12665452e-01 -1.02067748e-02  1.39323566e-02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc9gAUVfR8GP"
      },
      "source": [
        "# Now we will try and find the closest sentences to the overall document embedding and use that to show the summary of the sentence "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCLdCt0zEnDA",
        "outputId": "7dc498b0-977a-4e64-d3d0-bcd8a549c382"
      },
      "source": [
        "from scipy import spatial \n",
        "\n",
        "# We choose A lecture that talks about What makes an AI Company ? \n",
        "test_doc = doc_dict[81] \n",
        "test_dict = doc_matrix_dict[81]\n",
        "doc_contents = all_sentences[80]\n",
        "print(\"Lecture Title:\\n\",data.loc[80,\"lecture_topic\"])\n",
        "print(\"\\n\\nLecture Contents\\n\",doc_contents)\n",
        "print(test_doc)"
      ],
      "execution_count": 478,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lecture Title:\n",
            " what-makes-an-ai-company\n",
            "\n",
            "\n",
            "Lecture Contents\n",
            " ['What makes a company good at AI?', 'Perhaps even more importantly, what will it take for your company to become great at using AI?', \"I had previously led the Google brain team, and Baidu's AI group, which I respectively helped Google and Baidu become great AI companies.\", 'So, what can you do for your company?', 'This is the lesson I had learned to washing the rise of the Internet that I think will be relevant to how all of us navigate the rise of AI.', \"Let's take a look.\", 'A lesson we learned from the rise of the Internet was that, if you take your favorite shopping mall.', 'So, my wife and I sometimes shop at Stanford shopping center and you build a website for the Shopping mall.', 'Maybe sell things on the website, that by itself does not turn the shopping mall into an internet company.', 'In fact, a few years ago I was speaking with the CEO of a large retail company who said to me, \"Hey Andrew, I have a website, I sell things in the website.\"', 'Amazon has a website, Amazon sells things on website is the same thing.', \"But of course it wasn't, in the shopping mall with a website isn't the same thing as a first-class internet company.\", \"So, what is it that defines an internet company if it isn't just whether or not you sell things on the website?\", 'I think an Internet company is a company that does the thing that internet let you do really well.', 'For example, we engage and pervasive AB testing.', 'Meaning we routinely threw up two different versions of website and see which one works better because we can.', 'So, we learn much faster.', 'Whereas in a traditional shopping mall, very difficult to have two shopping malls in two parallel universes and you can only maybe change things around every quarter or every six months.', 'Internet company is since a very short iteration times.', 'You can ship a new product every week or maybe even every day because you can whereas a shopping mall can be redesigned and we are protected only every several months.', 'Internet companies also tend to push decision-making down from the CEO to the engineers and to other specialized rules such that the product managers.', 'This is in contrast to a traditional shopping mall.', 'We can maybe have the CEO just decide all the key decisions and then just everyone does what the CEO says.', \"It turns out that traditional model doesn't work in the internet era because only the engineers and other specialized roles like product managers know enough about the technology and the product and the users to make great decisions.\", \"So, these are some of the things that internet companies do in order to make sure they do the things that the internet doesn't do really well.\", 'This is a lesson we learned from the internet era.', 'How about the AI era?', 'I think that today, you can take any company and have it use a few neural networks or few deep learning algorithms.', 'That by itself does not turn the accompany into an AI company.', 'Instead, what makes a great AI company, sometimes an AI first company is, are you doing the things that AI lets you do really well?', 'For example, AI companies are very good at strategic data acquisition.', 'This is why many of the large consumer tech companies may have three products that do not monetize and it allows them to acquire data that they can monetize elsewhere.', 'Serve less strategy teams where we would deliberately launch products that do not make any money just for the sake of data acquisition.', 'Thinking through how to get data is a key part of the great AI companies.', 'AI companies sends a unified data warehouses.', 'If you have 50 different databases or 50 different data warehouses under the control of 50 different Vice-Presidents, then there will be impossible for an engineer to get the data into one place so that they can connect the dots and spot the patterns.', 'So, many great AI companies have preemptively invested in bringing the data together into single data warehouse to increase the odds that the teams can connect the dots.', 'Subject of course to privacy guarantees and also to data regulations such as GDPR in Europe.', 'AI companies are very good at spotting automation opportunities.', \"We're very good at saying, Oh!\", \"Let's insert the supervised learning algorithm and have an ATP mapping here so that we don't have to have people do these tasks instead we can automate It.\", 'AI companies also have many new roles such as the MLE or Machine Learning Engineer and new ways of dividing up tasks among different members of a team.', 'So, for a company to become good at AI means, architecting for company to do the things that AI makes it possible to do really well.', 'Now, for a company that become good at AI does require a process.', \"In fact, 10 years ago, Google and Baidu as well as companies like Facebook and Microsoft that I was not a part of, we're not great AI companies the way they are today.\", 'So, how can a company become good at AI?', 'It turns out that becoming good at AI is not a mysterious magical process.', 'Instead there is a systematic process through which many companies, almost any big company can become good at AI.', 'This is the five-step AI transformation playbook that I recommend to companies that want to become effective at using AI.', \"I'll give a brief overview of the playbook here and they're going to detail in a later week.\", 'Step one is to execute pilot projects to gain momentum.', 'So, just to a few small projects to get a better sense of what AI can or cannot do and get a better sense of what doing an AI project feels like.', 'This you could do in house or you can also do with an outsource team.', 'But eventually, you then need to do step two which is the building in house AI team and provide broad AI training, not just to the engineers but also to the managers, division leaders and executives and how they think about AI.', \"After doing this or as you're doing this, you have a better sense of what AI is and then is important for many companies to develop an AI strategy.\", 'Finally, to align internal and external communications so that all your stakeholders from employees, customers and investors are aligns with how your company is navigating the rise of AI.', 'AI has created tremendous value in the software industry and will continue to do so.', 'It will also create tremendous value outside the software industry.', 'If you can help your company become good at AI, I hope you can play a leading role in creating a lot of this value.', \"It is video you saw what is it that makes a company a good AI company and also briefly the AI transformation playbook which I'll go into much greater detail on in a later week as a road-map for helping companies become great at AI.\", \"If you're interested, there is also published online an AI transformation playbook that goes into these five steps in greater detail for you see more of these in the later weeks as well.\", 'Now, one of the challenges of doing AI projects such as the pilot projects in step one is understanding what AI can and cannot do.', 'In the next video, I want to show you and give you some examples of what AI can and cannot do, to help you better select projects AI that there may be effective for your company.', \"That's gone to the next video.\"]\n",
            "[['what', 'makes', 'a', 'company', 'good', 'at', 'ai', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['perhaps', 'even', 'more', 'importantly', 'what', 'will', 'it', 'take', 'for', 'your', 'company', 'to', 'become', 'great', 'at', 'using', 'ai', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['i', 'had', 'previously', 'led', 'the', 'google', 'brain', 'team', 'and', \"baidu's\", 'ai', 'group', 'which', 'i', 'respectively', 'helped', 'google', 'and', 'baidu', 'become', 'great', 'ai', 'companies', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['so', 'what', 'can', 'you', 'do', 'for', 'your', 'company', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['this', 'is', 'the', 'lesson', 'i', 'had', 'learned', 'to', 'washing', 'the', 'rise', 'of', 'the', 'internet', 'that', 'i', 'think', 'will', 'be', 'relevant', 'to', 'how', 'all', 'of', 'us', 'navigate', 'the', 'rise', 'of', 'ai', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], [\"let's\", 'take', 'a', 'look', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['a', 'lesson', 'we', 'learned', 'from', 'the', 'rise', 'of', 'the', 'internet', 'was', 'that', 'if', 'you', 'take', 'your', 'favorite', 'shopping', 'mall', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['so', 'my', 'wife', 'and', 'i', 'sometimes', 'shop', 'at', 'stanford', 'shopping', 'center', 'and', 'you', 'build', 'a', 'website', 'for', 'the', 'shopping', 'mall', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['maybe', 'sell', 'things', 'on', 'the', 'website', 'that', 'by', 'itself', 'does', 'not', 'turn', 'the', 'shopping', 'mall', 'into', 'an', 'internet', 'company', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['in', 'fact', 'a', 'few', 'years', 'ago', 'i', 'was', 'speaking', 'with', 'the', 'ceo', 'of', 'a', 'large', 'retail', 'company', 'who', 'said', 'to', 'me', 'hey', 'andrew', 'i', 'have', 'a', 'website', 'i', 'sell', 'things', 'in', 'the', 'website', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['amazon', 'has', 'a', 'website', 'amazon', 'sells', 'things', 'on', 'website', 'is', 'the', 'same', 'thing', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['but', 'of', 'course', 'it', \"wasn't\", 'in', 'the', 'shopping', 'mall', 'with', 'a', 'website', \"isn't\", 'the', 'same', 'thing', 'as', 'a', 'first', 'class', 'internet', 'company', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['so', 'what', 'is', 'it', 'that', 'defines', 'an', 'internet', 'company', 'if', 'it', \"isn't\", 'just', 'whether', 'or', 'not', 'you', 'sell', 'things', 'on', 'the', 'website', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['i', 'think', 'an', 'internet', 'company', 'is', 'a', 'company', 'that', 'does', 'the', 'thing', 'that', 'internet', 'let', 'you', 'do', 'really', 'well', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['for', 'example', 'we', 'engage', 'and', 'pervasive', 'ab', 'testing', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['meaning', 'we', 'routinely', 'threw', 'up', 'two', 'different', 'versions', 'of', 'website', 'and', 'see', 'which', 'one', 'works', 'better', 'because', 'we', 'can', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['so', 'we', 'learn', 'much', 'faster', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['whereas', 'in', 'a', 'traditional', 'shopping', 'mall', 'very', 'difficult', 'to', 'have', 'two', 'shopping', 'malls', 'in', 'two', 'parallel', 'universes', 'and', 'you', 'can', 'only', 'maybe', 'change', 'things', 'around', 'every', 'quarter', 'or', 'every', 'six', 'months', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['internet', 'company', 'is', 'since', 'a', 'very', 'short', 'iteration', 'times', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['you', 'can', 'ship', 'a', 'new', 'product', 'every', 'week', 'or', 'maybe', 'even', 'every', 'day', 'because', 'you', 'can', 'whereas', 'a', 'shopping', 'mall', 'can', 'be', 'redesigned', 'and', 'we', 'are', 'protected', 'only', 'every', 'several', 'months', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['internet', 'companies', 'also', 'tend', 'to', 'push', 'decision', 'making', 'down', 'from', 'the', 'ceo', 'to', 'the', 'engineers', 'and', 'to', 'other', 'specialized', 'rules', 'such', 'that', 'the', 'product', 'managers', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['this', 'is', 'in', 'contrast', 'to', 'a', 'traditional', 'shopping', 'mall', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['we', 'can', 'maybe', 'have', 'the', 'ceo', 'just', 'decide', 'all', 'the', 'key', 'decisions', 'and', 'then', 'just', 'everyone', 'does', 'what', 'the', 'ceo', 'says', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['it', 'turns', 'out', 'that', 'traditional', 'model', \"doesn't\", 'work', 'in', 'the', 'internet', 'era', 'because', 'only', 'the', 'engineers', 'and', 'other', 'specialized', 'roles', 'like', 'product', 'managers', 'know', 'enough', 'about', 'the', 'technology', 'and', 'the', 'product', 'and', 'the', 'users', 'to', 'make', 'great', 'decisions', '', '', '', '', '', '', '', '', '', '', '', ''], ['so', 'these', 'are', 'some', 'of', 'the', 'things', 'that', 'internet', 'companies', 'do', 'in', 'order', 'to', 'make', 'sure', 'they', 'do', 'the', 'things', 'that', 'the', 'internet', \"doesn't\", 'do', 'really', 'well', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['this', 'is', 'a', 'lesson', 'we', 'learned', 'from', 'the', 'internet', 'era', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['how', 'about', 'the', 'ai', 'era', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['i', 'think', 'that', 'today', 'you', 'can', 'take', 'any', 'company', 'and', 'have', 'it', 'use', 'a', 'few', 'neural', 'networks', 'or', 'few', 'deep', 'learning', 'algorithms', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['that', 'by', 'itself', 'does', 'not', 'turn', 'the', 'accompany', 'into', 'an', 'ai', 'company', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['instead', 'what', 'makes', 'a', 'great', 'ai', 'company', 'sometimes', 'an', 'ai', 'first', 'company', 'is', 'are', 'you', 'doing', 'the', 'things', 'that', 'ai', 'lets', 'you', 'do', 'really', 'well', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['for', 'example', 'ai', 'companies', 'are', 'very', 'good', 'at', 'strategic', 'data', 'acquisition', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['this', 'is', 'why', 'many', 'of', 'the', 'large', 'consumer', 'tech', 'companies', 'may', 'have', 'three', 'products', 'that', 'do', 'not', 'monetize', 'and', 'it', 'allows', 'them', 'to', 'acquire', 'data', 'that', 'they', 'can', 'monetize', 'elsewhere', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['serve', 'less', 'strategy', 'teams', 'where', 'we', 'would', 'deliberately', 'launch', 'products', 'that', 'do', 'not', 'make', 'any', 'money', 'just', 'for', 'the', 'sake', 'of', 'data', 'acquisition', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['thinking', 'through', 'how', 'to', 'get', 'data', 'is', 'a', 'key', 'part', 'of', 'the', 'great', 'ai', 'companies', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['ai', 'companies', 'sends', 'a', 'unified', 'data', 'warehouses', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['if', 'you', 'have', '50', 'different', 'databases', 'or', '50', 'different', 'data', 'warehouses', 'under', 'the', 'control', 'of', '50', 'different', 'vice', 'presidents', 'then', 'there', 'will', 'be', 'impossible', 'for', 'an', 'engineer', 'to', 'get', 'the', 'data', 'into', 'one', 'place', 'so', 'that', 'they', 'can', 'connect', 'the', 'dots', 'and', 'spot', 'the', 'patterns', '', '', '', '', ''], ['so', 'many', 'great', 'ai', 'companies', 'have', 'preemptively', 'invested', 'in', 'bringing', 'the', 'data', 'together', 'into', 'single', 'data', 'warehouse', 'to', 'increase', 'the', 'odds', 'that', 'the', 'teams', 'can', 'connect', 'the', 'dots', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['subject', 'of', 'course', 'to', 'privacy', 'guarantees', 'and', 'also', 'to', 'data', 'regulations', 'such', 'as', 'gdpr', 'in', 'europe', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['ai', 'companies', 'are', 'very', 'good', 'at', 'spotting', 'automation', 'opportunities', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], [\"we're\", 'very', 'good', 'at', 'saying', 'oh', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], [\"let's\", 'insert', 'the', 'supervised', 'learning', 'algorithm', 'and', 'have', 'an', 'atp', 'mapping', 'here', 'so', 'that', 'we', \"don't\", 'have', 'to', 'have', 'people', 'do', 'these', 'tasks', 'instead', 'we', 'can', 'automate', 'it', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['ai', 'companies', 'also', 'have', 'many', 'new', 'roles', 'such', 'as', 'the', 'mle', 'or', 'machine', 'learning', 'engineer', 'and', 'new', 'ways', 'of', 'dividing', 'up', 'tasks', 'among', 'different', 'members', 'of', 'a', 'team', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['so', 'for', 'a', 'company', 'to', 'become', 'good', 'at', 'ai', 'means', 'architecting', 'for', 'company', 'to', 'do', 'the', 'things', 'that', 'ai', 'makes', 'it', 'possible', 'to', 'do', 'really', 'well', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['now', 'for', 'a', 'company', 'that', 'become', 'good', 'at', 'ai', 'does', 'require', 'a', 'process', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['in', 'fact', '10', 'years', 'ago', 'google', 'and', 'baidu', 'as', 'well', 'as', 'companies', 'like', 'facebook', 'and', 'microsoft', 'that', 'i', 'was', 'not', 'a', 'part', 'of', \"we're\", 'not', 'great', 'ai', 'companies', 'the', 'way', 'they', 'are', 'today', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['so', 'how', 'can', 'a', 'company', 'become', 'good', 'at', 'ai', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['it', 'turns', 'out', 'that', 'becoming', 'good', 'at', 'ai', 'is', 'not', 'a', 'mysterious', 'magical', 'process', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['instead', 'there', 'is', 'a', 'systematic', 'process', 'through', 'which', 'many', 'companies', 'almost', 'any', 'big', 'company', 'can', 'become', 'good', 'at', 'ai', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['this', 'is', 'the', 'five', 'step', 'ai', 'transformation', 'playbook', 'that', 'i', 'recommend', 'to', 'companies', 'that', 'want', 'to', 'become', 'effective', 'at', 'using', 'ai', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], [\"i'll\", 'give', 'a', 'brief', 'overview', 'of', 'the', 'playbook', 'here', 'and', \"they're\", 'going', 'to', 'detail', 'in', 'a', 'later', 'week', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['step', 'one', 'is', 'to', 'execute', 'pilot', 'projects', 'to', 'gain', 'momentum', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['so', 'just', 'to', 'a', 'few', 'small', 'projects', 'to', 'get', 'a', 'better', 'sense', 'of', 'what', 'ai', 'can', 'or', 'cannot', 'do', 'and', 'get', 'a', 'better', 'sense', 'of', 'what', 'doing', 'an', 'ai', 'project', 'feels', 'like', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['this', 'you', 'could', 'do', 'in', 'house', 'or', 'you', 'can', 'also', 'do', 'with', 'an', 'outsource', 'team', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['but', 'eventually', 'you', 'then', 'need', 'to', 'do', 'step', 'two', 'which', 'is', 'the', 'building', 'in', 'house', 'ai', 'team', 'and', 'provide', 'broad', 'ai', 'training', 'not', 'just', 'to', 'the', 'engineers', 'but', 'also', 'to', 'the', 'managers', 'division', 'leaders', 'and', 'executives', 'and', 'how', 'they', 'think', 'about', 'ai', '', '', '', '', '', '', '', ''], ['after', 'doing', 'this', 'or', 'as', \"you're\", 'doing', 'this', 'you', 'have', 'a', 'better', 'sense', 'of', 'what', 'ai', 'is', 'and', 'then', 'is', 'important', 'for', 'many', 'companies', 'to', 'develop', 'an', 'ai', 'strategy', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['finally', 'to', 'align', 'internal', 'and', 'external', 'communications', 'so', 'that', 'all', 'your', 'stakeholders', 'from', 'employees', 'customers', 'and', 'investors', 'are', 'aligns', 'with', 'how', 'your', 'company', 'is', 'navigating', 'the', 'rise', 'of', 'ai', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['ai', 'has', 'created', 'tremendous', 'value', 'in', 'the', 'software', 'industry', 'and', 'will', 'continue', 'to', 'do', 'so', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['it', 'will', 'also', 'create', 'tremendous', 'value', 'outside', 'the', 'software', 'industry', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['if', 'you', 'can', 'help', 'your', 'company', 'become', 'good', 'at', 'ai', 'i', 'hope', 'you', 'can', 'play', 'a', 'leading', 'role', 'in', 'creating', 'a', 'lot', 'of', 'this', 'value', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['it', 'is', 'video', 'you', 'saw', 'what', 'is', 'it', 'that', 'makes', 'a', 'company', 'a', 'good', 'ai', 'company', 'and', 'also', 'briefly', 'the', 'ai', 'transformation', 'playbook', 'which', \"i'll\", 'go', 'into', 'much', 'greater', 'detail', 'on', 'in', 'a', 'later', 'week', 'as', 'a', 'road', 'map', 'for', 'helping', 'companies', 'become', 'great', 'at', 'ai', '', '', '', ''], ['if', \"you're\", 'interested', 'there', 'is', 'also', 'published', 'online', 'an', 'ai', 'transformation', 'playbook', 'that', 'goes', 'into', 'these', 'five', 'steps', 'in', 'greater', 'detail', 'for', 'you', 'see', 'more', 'of', 'these', 'in', 'the', 'later', 'weeks', 'as', 'well', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['now', 'one', 'of', 'the', 'challenges', 'of', 'doing', 'ai', 'projects', 'such', 'as', 'the', 'pilot', 'projects', 'in', 'step', 'one', 'is', 'understanding', 'what', 'ai', 'can', 'and', 'cannot', 'do', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['in', 'the', 'next', 'video', 'i', 'want', 'to', 'show', 'you', 'and', 'give', 'you', 'some', 'examples', 'of', 'what', 'ai', 'can', 'and', 'cannot', 'do', 'to', 'help', 'you', 'better', 'select', 'projects', 'ai', 'that', 'there', 'may', 'be', 'effective', 'for', 'your', 'company', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], [\"that's\", 'gone', 'to', 'the', 'next', 'video', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNkg5b5EVGBO"
      },
      "source": [
        "### We compute cosine similarity of each sentence embedding with the overall document embedding and return the top 3 sentences as the summary "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs2SwQ9DF7bF"
      },
      "source": [
        "demo_sent_list = []\n",
        "for k,v in test_dict.items():\n",
        "  if k == \"document_embedding\":\n",
        "    demo_doc_embed = v\n",
        "  else: \n",
        "    demo_sent_list.append(v)"
      ],
      "execution_count": 479,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iKlSu6tUvla"
      },
      "source": [
        "demo_sent_similarity = []\n",
        "for sent in demo_sent_list:\n",
        "  result = 1 - spatial.distance.cosine(demo_doc_embed, sent)\n",
        "  demo_sent_similarity.append(result)"
      ],
      "execution_count": 480,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdLqoMxBW6nd"
      },
      "source": [
        "demo_sent_index = [i for i in range(len(demo_sent_list))]"
      ],
      "execution_count": 481,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8DSdy4qV6_R"
      },
      "source": [
        "demo = pd.DataFrame()\n",
        "demo[\"Sentence_Number\"] = demo_sent_index\n",
        "demo[\"Similarity\"] = demo_sent_similarity\n"
      ],
      "execution_count": 482,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "hKfnig7_XpSs",
        "outputId": "4c417eae-b07f-491c-bf0c-aad68c51e15f"
      },
      "source": [
        "demo"
      ],
      "execution_count": 483,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence_Number</th>\n",
              "      <th>Similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.933812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.981189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.928872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.945198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.977994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>59</td>\n",
              "      <td>0.983464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>60</td>\n",
              "      <td>0.978879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>61</td>\n",
              "      <td>0.971466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>62</td>\n",
              "      <td>0.979675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>63</td>\n",
              "      <td>0.916477</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>64 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Sentence_Number  Similarity\n",
              "0                 0    0.933812\n",
              "1                 1    0.981189\n",
              "2                 2    0.928872\n",
              "3                 3    0.945198\n",
              "4                 4    0.977994\n",
              "..              ...         ...\n",
              "59               59    0.983464\n",
              "60               60    0.978879\n",
              "61               61    0.971466\n",
              "62               62    0.979675\n",
              "63               63    0.916477\n",
              "\n",
              "[64 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 483
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "ucesFID4WwEB",
        "outputId": "c02850fa-ea1d-43fd-ac4b-d08292aee04f"
      },
      "source": [
        "demo_sorted = demo.sort_values(\"Similarity\", ascending=False)\n",
        "demo_sorted"
      ],
      "execution_count": 484,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence_Number</th>\n",
              "      <th>Similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>54</td>\n",
              "      <td>0.988777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>42</td>\n",
              "      <td>0.984524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>59</td>\n",
              "      <td>0.983464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.981189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>51</td>\n",
              "      <td>0.980456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>38</td>\n",
              "      <td>0.877669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>0.871399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>0.857027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>39</td>\n",
              "      <td>0.851464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>34</td>\n",
              "      <td>0.786228</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>64 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Sentence_Number  Similarity\n",
              "54               54    0.988777\n",
              "42               42    0.984524\n",
              "59               59    0.983464\n",
              "1                 1    0.981189\n",
              "51               51    0.980456\n",
              "..              ...         ...\n",
              "38               38    0.877669\n",
              "26               26    0.871399\n",
              "14               14    0.857027\n",
              "39               39    0.851464\n",
              "34               34    0.786228\n",
              "\n",
              "[64 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 484
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmX_W1DoY8Kc",
        "outputId": "b48bd484-90c5-426a-8d73-a9b086fd1f43"
      },
      "source": [
        "len(doc_dict[81])"
      ],
      "execution_count": 485,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 485
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "id": "SM-0tjNUXr3P",
        "outputId": "5ff37859-ff51-4702-cdf2-4d3821040466"
      },
      "source": [
        "import re\n",
        "sent_idx_1 = demo_sorted.loc[1,\"Sentence_Number\"]\n",
        "sent_idx_2 = demo_sorted.loc[2,\"Sentence_Number\"]\n",
        "sent_idx_3 = demo_sorted.loc[3,\"Sentence_Number\"]\n",
        "\n",
        "final_sentences = test_doc[sent_idx_1] + test_doc[sent_idx_2] + test_doc[sent_idx_3]\n",
        "final_summary = \" \".join(final_sentences)\n",
        "final_summary_1 = re.sub(' {2,}', '. ', final_summary)\n",
        "print(\"Final Summary for the Document:\\n\")\n",
        "final_summary_1"
      ],
      "execution_count": 486,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Summary for the Document:\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"perhaps even more importantly what will it take for your company to become great at using ai. i had previously led the google brain team and baidu's ai group which i respectively helped google and baidu become great ai companies. so what can you do for your company. \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 486
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5n6fcqYcspT"
      },
      "source": [
        "# Hence we have our final summary ready and it seems to do an okay job"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bb3nZmT3dENo"
      },
      "source": [
        "### Lets have a look at a couple of more examples "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSNIH5bPdDb7",
        "outputId": "a2de3a3a-df48-407c-e5b8-0f0da7c52983"
      },
      "source": [
        "# We choose Another lecture\n",
        "test_doc = doc_dict[122] \n",
        "test_dict = doc_matrix_dict[122]\n",
        "doc_contents = all_sentences[121]\n",
        "print(\"Lecture Title:\\n\",data.loc[121,\"lecture_topic\"])\n",
        "print(\"\\n\\nLecture Contents\\n\",doc_contents)\n",
        "print(test_doc)"
      ],
      "execution_count": 464,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lecture Title:\n",
            " training-neural-networks\n",
            "\n",
            "\n",
            "Lecture Contents\n",
            " ['Now that you have dived a little deeper into neural networks.', \"Let's sort of how we can train them, some common pitfalls and something techniques to help speed up train and provide better journalism.\", 'In TensorFlow using the Estimator API, using a DNNRegressor, is very similar to using a LinearRegressor, with only a few parameters for the code that need to be added.', 'We can use momentum based optimizers such as the default Adagrad, or we can try many others such as Adam.', 'Also we now have to add a parameter named hidden units, which is a list.', 'The number of items in this list is the number of hidden layers and the values of each list item is a number of neurons for that particular hidden layer.', 'You will also know there is a new parameter named dropout.', \"We'll cover this and more in a few minutes.\", 'But for now this is used to turn individual neurons on and off for each example in hopes of having better generalization performance.', 'Please look at the tentacle documentation for the complete set of parameters you can configure.', 'These are all things that could be hyperparameterized so that you can tune your model to have the best generalization performance.', 'Back propagation is one of the traditional topics in an ML Neural Networks course.', \"But at some level, it's kind of like teaching people how to build a compiler.\", \"It's essential for deeper understanding but not necessarily needed for initial understanding.\", 'The main thing to know is that there is an efficient algorithm for calculating derivatives and TensorFlow will do it for you automatically.', 'There are some interesting failure cases to talk about though, such as vanishing gradients, exploding gradients and dead layers.', 'First, during the training process especially for deep networks gradients can vanish, each additional layer in your network can successively reduce signal vs noise.', 'An example of this is when using sigmoid or tanh activation functions throughout your hidden layers.', 'As you begin to saturate you end up in the asymptotic regions of the function which begin to plateau, the slope is getting closer and closer to approximately zero.', \"When you go backwards through the network during back prop, your gradient can become smaller and smaller because you're compounding all these small gradients until the gradient completely vanishes.\", 'When this happens your weights are no longer updating and therefore training grinds to a halt.', 'A simple way to fix this is to use non saturating non-linear activation functions such as ReLUs, ELUs, et cetera.', 'Next, we can also have the opposite problem where gradients explode, by getting bigger and bigger until our weights gets so large we overflow.', 'Even starting with relatively small gradients, such as a value of two, can compound and become quite large over many layers.', 'This is especially true for sequence models with long sequence lengths, learning rates can be a factor here because in our weight updates, remember we multiplied the gradient with the learning rate and then subtract that from the current weight.', \"So, even if the grading isn't that big with a learning rate greater than one it can now become too big and cause problems for us and our network.\", 'There are many techniques to try and minimize this.', 'Such as weight organization and smaller batch sizes.', 'Another technique is grading and clipping, where we check to see if the normal the gradient exceeds some threshold, which you can hyperparameter or tune and if so, then you can re-scale the gradient components to fit below your maximum.', 'Another useful technique is batch normalization which solves the problem called internal co-variance shift.', \"It's piece of training because gradients flow better.\", 'It also can often use a higher learning rate and might be able to get rid of dropout which slows competition down to its own kind of regularization due to mini batch noise.', \"To perform batch normalization, first find the mini batch mean, then the mini batch's standard deviation, then normalize the inputs to that node, then scale and shift by gamma times X plus beta, where gamma and beta are learned parameters.\", 'If gamma equals the square root variance of X and beta equals the mean of X, the original activation function is restored.', \"This way, you can control the range of your inputs so that they don't become too large.\", 'Ideally, you would like to keep your gradients as close to one as possible especially for very deep nets.', \"So you don't compound and eventually underflow or overflow.\", 'Another common failure mode of grading descent is that real layers can die.', 'Fortunately, using TensorBoard we can monitor the sun rays during and after training of our Neural Network models.', 'If using a candy and an estimator is automatically a scalar summary said for each GN hidden layer showing the fraction of zero values of the activations for that layer.', 'ReLUs stop working when their inputs keep them in the negative domain giving their activation a value of zero.', \"It doesn't end there because then their contribution in the next layer is zero, because despite what the weights are connecting it to the next neurons it's activation is zero thus the input becomes zero.\", \"A bunch of zeros come into the next neuron doesn't help it get into the positive domain and then these neurons activations become zero too and the problem continues to cascade.\", \"Then we perform back prop and their gradients are zero, so we don't have the weights and thus training halts.\", 'Not good.', \"We've talked about using Leaky or parametric ReLUs or even the slower ELUs, but you can also lower your learning rates to help stop ReLu layers from not activating and not staying.\", 'A large gradient possibly due to too high of a learning rate can update the weights in such a way that no data point will ever activate it again.', \"Since the gradient is zero, we won't update the weight to something more reasonable so the problem will persist indefinitely.\", \"Let's have a quick intuition check, what will happen to our model, if we have two useful signals both independently correlated with the label but there are at different scales?\", 'For example, we might have a soup deliciousness predictor where features represent qualities of giving ingredients.', 'If the feature for chicken stock is measured in liters, but beef stock is measured in milliliters then stochastic grading the scent might have a hard time converging well.', 'Since the optimal learning rate for these two dimensions is likely different.', 'Having your data clean and in a computationally helpful range has many benefits during the training process of your machine learning models.', 'Having feature value small and specifically zero centered helps speed up training and avoids numerical issues.', 'This is why batch normalization was helpful with exploding gradients because it made sure to keep not just the initial input features, but all of the intermediate features within a healthy range as not to cause problems with our layers.', 'This also helps us avoid the NaN trap, where our model can blow up if values exceed numerical precision range.', 'A combination of features scaling and/or lower learning rate can help us avoid this nasty pitfall.', 'Also, avoiding outlier values helps with generalization.', 'So detecting these perhaps the anomaly detection and pre-processing them out of the data set before training can be a great help.', 'Remember that there is no one best one size fits all method for all data.', 'It is possible to think of good and bad cases for each of these approaches.', 'There are many methods to make our future value scale to small numbers.', 'First, there is linear scaling where you first find the minimum and maximum of the data.', 'Then for each value, we subtract the minimum and then divide by the difference of the maximum and minimum or range.', 'This will make all values between zero and one, where zero will be the minimum and one will be the maximum.', 'This is also called normalization.', 'There is also hard caping or clipping, where you set a minimum value and a maximum value.', 'For instance, if my minimum value is allowed to be negative seven and my maximum value is 10, then all values less than negative seven will become negative seven, and all values greater than 10 will become 10.', 'Log scaling is another method where you apply the logarithm function to your input data.', 'This is great when your data has huge range and you want to condense it down to be more about just the magnitude of the value.', 'Another method which we just talked about with batch normalization is standardization.', 'Here, you calculate the mean of your data and the standard deviation.', 'Once you have these two values, you subtract the mean from every data point and then divide with the standard deviation.', 'This way, your data becomes zero centered because your new mean become zero and your new standard deviation becomes one.', 'Of course, there are many other ways to scale your data.', 'Which of these is good advice if my model is experiencing exploding gradients?', 'The correct answer is A, B, C and D. The problem often occurs when weights get too large, which can happen when our learning rate gets too high.', 'This can lead to a whole bunch of other issues like numerical stability, divergence and [inaudible].', 'Therefore, lowering the learning rate to find that nice Goldilocks zone is a great idea.', 'Weight authorization can also help in this respect because there will be a penalty for very large weights, which should make it harder for gradients to explode.', 'Also, applying gradient clipping can ensure that gradients never get beyond a certain threshold that we set.', 'This can help mitigate somewhat a higher learning rate.', 'However, with a high enough rate, it can still drive the weights to very high values.', 'Batch normalization can help the intermediate inputs at each layer stay within a tighter range, so there will be a much reduced chance of weights growing out of range for a small extra computational cost.', \"There are many methods to treat exploding gradients, so you don't need a doctor to help.\", 'All you have to do is experiment with these tools and see what works best.', 'Another form of regularization that helps build more generalizable models is adding dropout layers to our neural networks.', 'To use dropout, I add a wrapper to one or more of my layers.', 'Intenser flow, the parameter you pass is called dropout, which is the probability of dropping a neuron temporarily from the network rather than keeping it turned on.', 'You want to be careful when setting this number because for some other functions that have a dropout mechanism, they use keep probability, which is a complement to drop probability or the probability of keeping a neuron on or off.', \"You wouldn't want to intend only a 10 percent probability to drop, but actually are now only keeping 10 percent in your nodes randomly; that's a very unintentional sparse model.\", 'So, how does dropout work under the hood?', \"Let's say we set a dropout probability of 20 percent.\", 'This means that for each forward parsed to the network, the algorithm will roll the dice for each neuron and the dropout wrapped layer.', 'If the dice roll is greater than 20 and the neuron will stay active in the network, [inaudible] roll will be dropped, and output a value of zero regardless of its inputs effectively not adding negatively or positively to the network.', \"Since adding zero changes nothing and simulates to the neuron doesn't exist.\", 'To make up for the fact that each node is only kept some percentage of the time, the activations are scaled by one over one minus the dropout probability or in other words, one over the keep probability during training so that it is the expectation value of the activation.', 'When not doing training without having to change any code, the wrapper effectively disappears and the neurons in the formally dropout wrapper layer are always on and use whatever weights were trained by the model.', 'The awesome idea of dropout is that it is essentially creating an ensemble model, because for each forward pass there is effectively a different network that the mini batch of data is seen.', 'When all this is added together in expectation, it is like I would train two to the n neural networks, where n is the number of dropout neurons, and have them working in an ensemble similar to a bunch of decision trees working together in a random forest.', 'There is also the added effect of spreading out the data distribution of the entire network, rather than having the majority of the signal favor going along one branch of the network.', \"I usually imagine this as diverting water in a river or stream with multiple shunts or dams to ensure all waterways eventually get some water and don't dry up.\", \"This way, your network uses more of its capacity since the signal more evenly flows across the entire network and thus, you'll have better training and generalization without large neuron dependencies being developed in popular paths.\", 'Typical values for dropout are between 20 to 50 percent.', 'If you go much lower than that, there is not much effect from the network since you are rarely dropping any nodes.', \"If you go higher, then training doesn't happen as well since the network becomes too sparse to have the capacity to learn without distribution.\", 'You also want to use this on larger networks because there is more capacity for the model to learn independent representations.', 'In other words, there are more possible pass for the network to try.', 'The more you drop out, therefore the less you keep, the stronger the regularization.', 'If you set your dropout probability to one, then you keep nothing and every neuron in the wrapped dropout layer is effectively removed from the neuron, and outputs a zero activation.', 'During backprop, this means that the weights will not update and this layer will learn nothing.', 'If you set your probably to zero, then all neurons are kept active and there is no dropout regularization.', \"It's pretty much just a more computationally costly way to not have a dropout wrapper at all because you still have to roll the dice.\", 'Of course, somewhere between zero and one is where you want to be.', 'Specifically with dropout probabilities between 10 to 50 percent, where a good baseline is usually starting at 20 percent and then adding more is needed.', 'There is no one-size-fits-all dropout probability for all models and all data distributions.', 'Dropout acts as another form of blank.', 'It forces data to flow down blank paths so that there is a more even spread.', 'It also simulates blank learning.', \"Don't forget to scale the dropout activations by the inverse of the blank.\", 'We remove dropout during blank.', 'The correct answer is E. Dropout act is another form of regularization so the model can generalize better.', 'It does this turning off nodes with a dropout probability, which forces data to flow down multiple paths so that there is a more even spread.', 'Otherwise, data and the activations associated with it can learn to take preferential paths, which might lead to under training of the network as a whole and provide poor performance on new data.', 'Dropout also simulates ensemble learning by creating an aggregate of two to the n models due to the random turning off of nodes for each forward pass, where n is the number of dropout nodes.', \"Each batch sees a different network, so the model can't overfit on the entire training set much like a random forest.\", \"Don't forget to scale the dropout activations by the inverse of the keep probability, which is one minus the dropout probability.\", 'We do this with the expectation on the node will be scaled correctly during training, since for inference, it will always be on; since we remove dropout during inference.']\n",
            "[['now', 'that', 'you', 'have', 'dived', 'a', 'little', 'deeper', 'into', 'neural', 'networks', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], [\"let's\", 'sort', 'of', 'how', 'we', 'can', 'train', 'them', 'some', 'common', 'pitfalls', 'and', 'something', 'techniques', 'to', 'help', 'speed', 'up', 'train', 'and', 'provide', 'better', 'journalism', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['in', 'tensorflow', 'using', 'the', 'estimator', 'api', 'using', 'a', 'dnnregressor', 'is', 'very', 'similar', 'to', 'using', 'a', 'linearregressor', 'with', 'only', 'a', 'few', 'parameters', 'for', 'the', 'code', 'that', 'need', 'to', 'be', 'added', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['we', 'can', 'use', 'momentum', 'based', 'optimizers', 'such', 'as', 'the', 'default', 'adagrad', 'or', 'we', 'can', 'try', 'many', 'others', 'such', 'as', 'adam', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['also', 'we', 'now', 'have', 'to', 'add', 'a', 'parameter', 'named', 'hidden', 'units', 'which', 'is', 'a', 'list', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['the', 'number', 'of', 'items', 'in', 'this', 'list', 'is', 'the', 'number', 'of', 'hidden', 'layers', 'and', 'the', 'values', 'of', 'each', 'list', 'item', 'is', 'a', 'number', 'of', 'neurons', 'for', 'that', 'particular', 'hidden', 'layer', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['you', 'will', 'also', 'know', 'there', 'is', 'a', 'new', 'parameter', 'named', 'dropout', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], [\"we'll\", 'cover', 'this', 'and', 'more', 'in', 'a', 'few', 'minutes', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['but', 'for', 'now', 'this', 'is', 'used', 'to', 'turn', 'individual', 'neurons', 'on', 'and', 'off', 'for', 'each', 'example', 'in', 'hopes', 'of', 'having', 'better', 'generalization', 'performance', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['please', 'look', 'at', 'the', 'tentacle', 'documentation', 'for', 'the', 'complete', 'set', 'of', 'parameters', 'you', 'can', 'configure', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['these', 'are', 'all', 'things', 'that', 'could', 'be', 'hyperparameterized', 'so', 'that', 'you', 'can', 'tune', 'your', 'model', 'to', 'have', 'the', 'best', 'generalization', 'performance', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['back', 'propagation', 'is', 'one', 'of', 'the', 'traditional', 'topics', 'in', 'an', 'ml', 'neural', 'networks', 'course', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['but', 'at', 'some', 'level', \"it's\", 'kind', 'of', 'like', 'teaching', 'people', 'how', 'to', 'build', 'a', 'compiler', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], [\"it's\", 'essential', 'for', 'deeper', 'understanding', 'but', 'not', 'necessarily', 'needed', 'for', 'initial', 'understanding', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['the', 'main', 'thing', 'to', 'know', 'is', 'that', 'there', 'is', 'an', 'efficient', 'algorithm', 'for', 'calculating', 'derivatives', 'and', 'tensorflow', 'will', 'do', 'it', 'for', 'you', 'automatically', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['there', 'are', 'some', 'interesting', 'failure', 'cases', 'to', 'talk', 'about', 'though', 'such', 'as', 'vanishing', 'gradients', 'exploding', 'gradients', 'and', 'dead', 'layers', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['first', 'during', 'the', 'training', 'process', 'especially', 'for', 'deep', 'networks', 'gradients', 'can', 'vanish', 'each', 'additional', 'layer', 'in', 'your', 'network', 'can', 'successively', 'reduce', 'signal', 'vs', 'noise', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['an', 'example', 'of', 'this', 'is', 'when', 'using', 'sigmoid', 'or', 'tanh', 'activation', 'functions', 'throughout', 'your', 'hidden', 'layers', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['as', 'you', 'begin', 'to', 'saturate', 'you', 'end', 'up', 'in', 'the', 'asymptotic', 'regions', 'of', 'the', 'function', 'which', 'begin', 'to', 'plateau', 'the', 'slope', 'is', 'getting', 'closer', 'and', 'closer', 'to', 'approximately', 'zero', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['when', 'you', 'go', 'backwards', 'through', 'the', 'network', 'during', 'back', 'prop', 'your', 'gradient', 'can', 'become', 'smaller', 'and', 'smaller', 'because', \"you're\", 'compounding', 'all', 'these', 'small', 'gradients', 'until', 'the', 'gradient', 'completely', 'vanishes', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['when', 'this', 'happens', 'your', 'weights', 'are', 'no', 'longer', 'updating', 'and', 'therefore', 'training', 'grinds', 'to', 'a', 'halt', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['a', 'simple', 'way', 'to', 'fix', 'this', 'is', 'to', 'use', 'non', 'saturating', 'non', 'linear', 'activation', 'functions', 'such', 'as', 'relus', 'elus', 'et', 'cetera', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['next', 'we', 'can', 'also', 'have', 'the', 'opposite', 'problem', 'where', 'gradients', 'explode', 'by', 'getting', 'bigger', 'and', 'bigger', 'until', 'our', 'weights', 'gets', 'so', 'large', 'we', 'overflow', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['even', 'starting', 'with', 'relatively', 'small', 'gradients', 'such', 'as', 'a', 'value', 'of', 'two', 'can', 'compound', 'and', 'become', 'quite', 'large', 'over', 'many', 'layers', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['this', 'is', 'especially', 'true', 'for', 'sequence', 'models', 'with', 'long', 'sequence', 'lengths', 'learning', 'rates', 'can', 'be', 'a', 'factor', 'here', 'because', 'in', 'our', 'weight', 'updates', 'remember', 'we', 'multiplied', 'the', 'gradient', 'with', 'the', 'learning', 'rate', 'and', 'then', 'subtract', 'that', 'from', 'the', 'current', 'weight', '', '', '', '', '', '', '', '', '', ''], ['so', 'even', 'if', 'the', 'grading', \"isn't\", 'that', 'big', 'with', 'a', 'learning', 'rate', 'greater', 'than', 'one', 'it', 'can', 'now', 'become', 'too', 'big', 'and', 'cause', 'problems', 'for', 'us', 'and', 'our', 'network', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['there', 'are', 'many', 'techniques', 'to', 'try', 'and', 'minimize', 'this', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['such', 'as', 'weight', 'organization', 'and', 'smaller', 'batch', 'sizes', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['another', 'technique', 'is', 'grading', 'and', 'clipping', 'where', 'we', 'check', 'to', 'see', 'if', 'the', 'normal', 'the', 'gradient', 'exceeds', 'some', 'threshold', 'which', 'you', 'can', 'hyperparameter', 'or', 'tune', 'and', 'if', 'so', 'then', 'you', 'can', 're', 'scale', 'the', 'gradient', 'components', 'to', 'fit', 'below', 'your', 'maximum', '', '', '', '', '', '', '', '', ''], ['another', 'useful', 'technique', 'is', 'batch', 'normalization', 'which', 'solves', 'the', 'problem', 'called', 'internal', 'co', 'variance', 'shift', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], [\"it's\", 'piece', 'of', 'training', 'because', 'gradients', 'flow', 'better', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['it', 'also', 'can', 'often', 'use', 'a', 'higher', 'learning', 'rate', 'and', 'might', 'be', 'able', 'to', 'get', 'rid', 'of', 'dropout', 'which', 'slows', 'competition', 'down', 'to', 'its', 'own', 'kind', 'of', 'regularization', 'due', 'to', 'mini', 'batch', 'noise', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['to', 'perform', 'batch', 'normalization', 'first', 'find', 'the', 'mini', 'batch', 'mean', 'then', 'the', 'mini', \"batch's\", 'standard', 'deviation', 'then', 'normalize', 'the', 'inputs', 'to', 'that', 'node', 'then', 'scale', 'and', 'shift', 'by', 'gamma', 'times', 'x', 'plus', 'beta', 'where', 'gamma', 'and', 'beta', 'are', 'learned', 'parameters', '', '', '', '', '', '', '', '', '', ''], ['if', 'gamma', 'equals', 'the', 'square', 'root', 'variance', 'of', 'x', 'and', 'beta', 'equals', 'the', 'mean', 'of', 'x', 'the', 'original', 'activation', 'function', 'is', 'restored', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['this', 'way', 'you', 'can', 'control', 'the', 'range', 'of', 'your', 'inputs', 'so', 'that', 'they', \"don't\", 'become', 'too', 'large', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['ideally', 'you', 'would', 'like', 'to', 'keep', 'your', 'gradients', 'as', 'close', 'to', 'one', 'as', 'possible', 'especially', 'for', 'very', 'deep', 'nets', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['so', 'you', \"don't\", 'compound', 'and', 'eventually', 'underflow', 'or', 'overflow', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['another', 'common', 'failure', 'mode', 'of', 'grading', 'descent', 'is', 'that', 'real', 'layers', 'can', 'die', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['fortunately', 'using', 'tensorboard', 'we', 'can', 'monitor', 'the', 'sun', 'rays', 'during', 'and', 'after', 'training', 'of', 'our', 'neural', 'network', 'models', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['if', 'using', 'a', 'candy', 'and', 'an', 'estimator', 'is', 'automatically', 'a', 'scalar', 'summary', 'said', 'for', 'each', 'gn', 'hidden', 'layer', 'showing', 'the', 'fraction', 'of', 'zero', 'values', 'of', 'the', 'activations', 'for', 'that', 'layer', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['relus', 'stop', 'working', 'when', 'their', 'inputs', 'keep', 'them', 'in', 'the', 'negative', 'domain', 'giving', 'their', 'activation', 'a', 'value', 'of', 'zero', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['it', \"doesn't\", 'end', 'there', 'because', 'then', 'their', 'contribution', 'in', 'the', 'next', 'layer', 'is', 'zero', 'because', 'despite', 'what', 'the', 'weights', 'are', 'connecting', 'it', 'to', 'the', 'next', 'neurons', \"it's\", 'activation', 'is', 'zero', 'thus', 'the', 'input', 'becomes', 'zero', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['a', 'bunch', 'of', 'zeros', 'come', 'into', 'the', 'next', 'neuron', \"doesn't\", 'help', 'it', 'get', 'into', 'the', 'positive', 'domain', 'and', 'then', 'these', 'neurons', 'activations', 'become', 'zero', 'too', 'and', 'the', 'problem', 'continues', 'to', 'cascade', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['then', 'we', 'perform', 'back', 'prop', 'and', 'their', 'gradients', 'are', 'zero', 'so', 'we', \"don't\", 'have', 'the', 'weights', 'and', 'thus', 'training', 'halts', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['not', 'good', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], [\"we've\", 'talked', 'about', 'using', 'leaky', 'or', 'parametric', 'relus', 'or', 'even', 'the', 'slower', 'elus', 'but', 'you', 'can', 'also', 'lower', 'your', 'learning', 'rates', 'to', 'help', 'stop', 'relu', 'layers', 'from', 'not', 'activating', 'and', 'not', 'staying', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['a', 'large', 'gradient', 'possibly', 'due', 'to', 'too', 'high', 'of', 'a', 'learning', 'rate', 'can', 'update', 'the', 'weights', 'in', 'such', 'a', 'way', 'that', 'no', 'data', 'point', 'will', 'ever', 'activate', 'it', 'again', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['since', 'the', 'gradient', 'is', 'zero', 'we', \"won't\", 'update', 'the', 'weight', 'to', 'something', 'more', 'reasonable', 'so', 'the', 'problem', 'will', 'persist', 'indefinitely', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], [\"let's\", 'have', 'a', 'quick', 'intuition', 'check', 'what', 'will', 'happen', 'to', 'our', 'model', 'if', 'we', 'have', 'two', 'useful', 'signals', 'both', 'independently', 'correlated', 'with', 'the', 'label', 'but', 'there', 'are', 'at', 'different', 'scales', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['for', 'example', 'we', 'might', 'have', 'a', 'soup', 'deliciousness', 'predictor', 'where', 'features', 'represent', 'qualities', 'of', 'giving', 'ingredients', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['if', 'the', 'feature', 'for', 'chicken', 'stock', 'is', 'measured', 'in', 'liters', 'but', 'beef', 'stock', 'is', 'measured', 'in', 'milliliters', 'then', 'stochastic', 'grading', 'the', 'scent', 'might', 'have', 'a', 'hard', 'time', 'converging', 'well', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['since', 'the', 'optimal', 'learning', 'rate', 'for', 'these', 'two', 'dimensions', 'is', 'likely', 'different', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['having', 'your', 'data', 'clean', 'and', 'in', 'a', 'computationally', 'helpful', 'range', 'has', 'many', 'benefits', 'during', 'the', 'training', 'process', 'of', 'your', 'machine', 'learning', 'models', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['having', 'feature', 'value', 'small', 'and', 'specifically', 'zero', 'centered', 'helps', 'speed', 'up', 'training', 'and', 'avoids', 'numerical', 'issues', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['this', 'is', 'why', 'batch', 'normalization', 'was', 'helpful', 'with', 'exploding', 'gradients', 'because', 'it', 'made', 'sure', 'to', 'keep', 'not', 'just', 'the', 'initial', 'input', 'features', 'but', 'all', 'of', 'the', 'intermediate', 'features', 'within', 'a', 'healthy', 'range', 'as', 'not', 'to', 'cause', 'problems', 'with', 'our', 'layers', '', '', '', '', '', '', '', '', '', ''], ['this', 'also', 'helps', 'us', 'avoid', 'the', 'nan', 'trap', 'where', 'our', 'model', 'can', 'blow', 'up', 'if', 'values', 'exceed', 'numerical', 'precision', 'range', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['a', 'combination', 'of', 'features', 'scaling', 'and', 'or', 'lower', 'learning', 'rate', 'can', 'help', 'us', 'avoid', 'this', 'nasty', 'pitfall', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['also', 'avoiding', 'outlier', 'values', 'helps', 'with', 'generalization', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['so', 'detecting', 'these', 'perhaps', 'the', 'anomaly', 'detection', 'and', 'pre', 'processing', 'them', 'out', 'of', 'the', 'data', 'set', 'before', 'training', 'can', 'be', 'a', 'great', 'help', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['remember', 'that', 'there', 'is', 'no', 'one', 'best', 'one', 'size', 'fits', 'all', 'method', 'for', 'all', 'data', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['it', 'is', 'possible', 'to', 'think', 'of', 'good', 'and', 'bad', 'cases', 'for', 'each', 'of', 'these', 'approaches', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['there', 'are', 'many', 'methods', 'to', 'make', 'our', 'future', 'value', 'scale', 'to', 'small', 'numbers', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['first', 'there', 'is', 'linear', 'scaling', 'where', 'you', 'first', 'find', 'the', 'minimum', 'and', 'maximum', 'of', 'the', 'data', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['then', 'for', 'each', 'value', 'we', 'subtract', 'the', 'minimum', 'and', 'then', 'divide', 'by', 'the', 'difference', 'of', 'the', 'maximum', 'and', 'minimum', 'or', 'range', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['this', 'will', 'make', 'all', 'values', 'between', 'zero', 'and', 'one', 'where', 'zero', 'will', 'be', 'the', 'minimum', 'and', 'one', 'will', 'be', 'the', 'maximum', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['this', 'is', 'also', 'called', 'normalization', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['there', 'is', 'also', 'hard', 'caping', 'or', 'clipping', 'where', 'you', 'set', 'a', 'minimum', 'value', 'and', 'a', 'maximum', 'value', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['for', 'instance', 'if', 'my', 'minimum', 'value', 'is', 'allowed', 'to', 'be', 'negative', 'seven', 'and', 'my', 'maximum', 'value', 'is', '10', 'then', 'all', 'values', 'less', 'than', 'negative', 'seven', 'will', 'become', 'negative', 'seven', 'and', 'all', 'values', 'greater', 'than', '10', 'will', 'become', '10', '', '', '', '', '', '', '', '', '', '', '', ''], ['log', 'scaling', 'is', 'another', 'method', 'where', 'you', 'apply', 'the', 'logarithm', 'function', 'to', 'your', 'input', 'data', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['this', 'is', 'great', 'when', 'your', 'data', 'has', 'huge', 'range', 'and', 'you', 'want', 'to', 'condense', 'it', 'down', 'to', 'be', 'more', 'about', 'just', 'the', 'magnitude', 'of', 'the', 'value', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['another', 'method', 'which', 'we', 'just', 'talked', 'about', 'with', 'batch', 'normalization', 'is', 'standardization', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['here', 'you', 'calculate', 'the', 'mean', 'of', 'your', 'data', 'and', 'the', 'standard', 'deviation', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['once', 'you', 'have', 'these', 'two', 'values', 'you', 'subtract', 'the', 'mean', 'from', 'every', 'data', 'point', 'and', 'then', 'divide', 'with', 'the', 'standard', 'deviation', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['this', 'way', 'your', 'data', 'becomes', 'zero', 'centered', 'because', 'your', 'new', 'mean', 'become', 'zero', 'and', 'your', 'new', 'standard', 'deviation', 'becomes', 'one', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['of', 'course', 'there', 'are', 'many', 'other', 'ways', 'to', 'scale', 'your', 'data', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['which', 'of', 'these', 'is', 'good', 'advice', 'if', 'my', 'model', 'is', 'experiencing', 'exploding', 'gradients', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['the', 'correct', 'answer', 'is', 'a', 'b', 'c', 'and', 'd', 'the', 'problem', 'often', 'occurs', 'when', 'weights', 'get', 'too', 'large', 'which', 'can', 'happen', 'when', 'our', 'learning', 'rate', 'gets', 'too', 'high', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['this', 'can', 'lead', 'to', 'a', 'whole', 'bunch', 'of', 'other', 'issues', 'like', 'numerical', 'stability', 'divergence', 'and', 'inaudible', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['therefore', 'lowering', 'the', 'learning', 'rate', 'to', 'find', 'that', 'nice', 'goldilocks', 'zone', 'is', 'a', 'great', 'idea', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['weight', 'authorization', 'can', 'also', 'help', 'in', 'this', 'respect', 'because', 'there', 'will', 'be', 'a', 'penalty', 'for', 'very', 'large', 'weights', 'which', 'should', 'make', 'it', 'harder', 'for', 'gradients', 'to', 'explode', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['also', 'applying', 'gradient', 'clipping', 'can', 'ensure', 'that', 'gradients', 'never', 'get', 'beyond', 'a', 'certain', 'threshold', 'that', 'we', 'set', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['this', 'can', 'help', 'mitigate', 'somewhat', 'a', 'higher', 'learning', 'rate', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['however', 'with', 'a', 'high', 'enough', 'rate', 'it', 'can', 'still', 'drive', 'the', 'weights', 'to', 'very', 'high', 'values', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['batch', 'normalization', 'can', 'help', 'the', 'intermediate', 'inputs', 'at', 'each', 'layer', 'stay', 'within', 'a', 'tighter', 'range', 'so', 'there', 'will', 'be', 'a', 'much', 'reduced', 'chance', 'of', 'weights', 'growing', 'out', 'of', 'range', 'for', 'a', 'small', 'extra', 'computational', 'cost', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['there', 'are', 'many', 'methods', 'to', 'treat', 'exploding', 'gradients', 'so', 'you', \"don't\", 'need', 'a', 'doctor', 'to', 'help', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['all', 'you', 'have', 'to', 'do', 'is', 'experiment', 'with', 'these', 'tools', 'and', 'see', 'what', 'works', 'best', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['another', 'form', 'of', 'regularization', 'that', 'helps', 'build', 'more', 'generalizable', 'models', 'is', 'adding', 'dropout', 'layers', 'to', 'our', 'neural', 'networks', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['to', 'use', 'dropout', 'i', 'add', 'a', 'wrapper', 'to', 'one', 'or', 'more', 'of', 'my', 'layers', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['intenser', 'flow', 'the', 'parameter', 'you', 'pass', 'is', 'called', 'dropout', 'which', 'is', 'the', 'probability', 'of', 'dropping', 'a', 'neuron', 'temporarily', 'from', 'the', 'network', 'rather', 'than', 'keeping', 'it', 'turned', 'on', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['you', 'want', 'to', 'be', 'careful', 'when', 'setting', 'this', 'number', 'because', 'for', 'some', 'other', 'functions', 'that', 'have', 'a', 'dropout', 'mechanism', 'they', 'use', 'keep', 'probability', 'which', 'is', 'a', 'complement', 'to', 'drop', 'probability', 'or', 'the', 'probability', 'of', 'keeping', 'a', 'neuron', 'on', 'or', 'off', '', '', '', '', '', '', '', '', '', ''], ['you', \"wouldn't\", 'want', 'to', 'intend', 'only', 'a', '10', 'percent', 'probability', 'to', 'drop', 'but', 'actually', 'are', 'now', 'only', 'keeping', '10', 'percent', 'in', 'your', 'nodes', 'randomly', \"that's\", 'a', 'very', 'unintentional', 'sparse', 'model', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['so', 'how', 'does', 'dropout', 'work', 'under', 'the', 'hood', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], [\"let's\", 'say', 'we', 'set', 'a', 'dropout', 'probability', 'of', '20', 'percent', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['this', 'means', 'that', 'for', 'each', 'forward', 'parsed', 'to', 'the', 'network', 'the', 'algorithm', 'will', 'roll', 'the', 'dice', 'for', 'each', 'neuron', 'and', 'the', 'dropout', 'wrapped', 'layer', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['if', 'the', 'dice', 'roll', 'is', 'greater', 'than', '20', 'and', 'the', 'neuron', 'will', 'stay', 'active', 'in', 'the', 'network', 'inaudible', 'roll', 'will', 'be', 'dropped', 'and', 'output', 'a', 'value', 'of', 'zero', 'regardless', 'of', 'its', 'inputs', 'effectively', 'not', 'adding', 'negatively', 'or', 'positively', 'to', 'the', 'network', '', '', '', '', '', '', '', '', ''], ['since', 'adding', 'zero', 'changes', 'nothing', 'and', 'simulates', 'to', 'the', 'neuron', \"doesn't\", 'exist', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['to', 'make', 'up', 'for', 'the', 'fact', 'that', 'each', 'node', 'is', 'only', 'kept', 'some', 'percentage', 'of', 'the', 'time', 'the', 'activations', 'are', 'scaled', 'by', 'one', 'over', 'one', 'minus', 'the', 'dropout', 'probability', 'or', 'in', 'other', 'words', 'one', 'over', 'the', 'keep', 'probability', 'during', 'training', 'so', 'that', 'it', 'is', 'the', 'expectation', 'value', 'of', 'the', 'activation'], ['when', 'not', 'doing', 'training', 'without', 'having', 'to', 'change', 'any', 'code', 'the', 'wrapper', 'effectively', 'disappears', 'and', 'the', 'neurons', 'in', 'the', 'formally', 'dropout', 'wrapper', 'layer', 'are', 'always', 'on', 'and', 'use', 'whatever', 'weights', 'were', 'trained', 'by', 'the', 'model', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['the', 'awesome', 'idea', 'of', 'dropout', 'is', 'that', 'it', 'is', 'essentially', 'creating', 'an', 'ensemble', 'model', 'because', 'for', 'each', 'forward', 'pass', 'there', 'is', 'effectively', 'a', 'different', 'network', 'that', 'the', 'mini', 'batch', 'of', 'data', 'is', 'seen', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['when', 'all', 'this', 'is', 'added', 'together', 'in', 'expectation', 'it', 'is', 'like', 'i', 'would', 'train', 'two', 'to', 'the', 'n', 'neural', 'networks', 'where', 'n', 'is', 'the', 'number', 'of', 'dropout', 'neurons', 'and', 'have', 'them', 'working', 'in', 'an', 'ensemble', 'similar', 'to', 'a', 'bunch', 'of', 'decision', 'trees', 'working', 'together', 'in', 'a', 'random', 'forest', '', ''], ['there', 'is', 'also', 'the', 'added', 'effect', 'of', 'spreading', 'out', 'the', 'data', 'distribution', 'of', 'the', 'entire', 'network', 'rather', 'than', 'having', 'the', 'majority', 'of', 'the', 'signal', 'favor', 'going', 'along', 'one', 'branch', 'of', 'the', 'network', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['i', 'usually', 'imagine', 'this', 'as', 'diverting', 'water', 'in', 'a', 'river', 'or', 'stream', 'with', 'multiple', 'shunts', 'or', 'dams', 'to', 'ensure', 'all', 'waterways', 'eventually', 'get', 'some', 'water', 'and', \"don't\", 'dry', 'up', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['this', 'way', 'your', 'network', 'uses', 'more', 'of', 'its', 'capacity', 'since', 'the', 'signal', 'more', 'evenly', 'flows', 'across', 'the', 'entire', 'network', 'and', 'thus', \"you'll\", 'have', 'better', 'training', 'and', 'generalization', 'without', 'large', 'neuron', 'dependencies', 'being', 'developed', 'in', 'popular', 'paths', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['typical', 'values', 'for', 'dropout', 'are', 'between', '20', 'to', '50', 'percent', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['if', 'you', 'go', 'much', 'lower', 'than', 'that', 'there', 'is', 'not', 'much', 'effect', 'from', 'the', 'network', 'since', 'you', 'are', 'rarely', 'dropping', 'any', 'nodes', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['if', 'you', 'go', 'higher', 'then', 'training', \"doesn't\", 'happen', 'as', 'well', 'since', 'the', 'network', 'becomes', 'too', 'sparse', 'to', 'have', 'the', 'capacity', 'to', 'learn', 'without', 'distribution', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['you', 'also', 'want', 'to', 'use', 'this', 'on', 'larger', 'networks', 'because', 'there', 'is', 'more', 'capacity', 'for', 'the', 'model', 'to', 'learn', 'independent', 'representations', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['in', 'other', 'words', 'there', 'are', 'more', 'possible', 'pass', 'for', 'the', 'network', 'to', 'try', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['the', 'more', 'you', 'drop', 'out', 'therefore', 'the', 'less', 'you', 'keep', 'the', 'stronger', 'the', 'regularization', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['if', 'you', 'set', 'your', 'dropout', 'probability', 'to', 'one', 'then', 'you', 'keep', 'nothing', 'and', 'every', 'neuron', 'in', 'the', 'wrapped', 'dropout', 'layer', 'is', 'effectively', 'removed', 'from', 'the', 'neuron', 'and', 'outputs', 'a', 'zero', 'activation', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['during', 'backprop', 'this', 'means', 'that', 'the', 'weights', 'will', 'not', 'update', 'and', 'this', 'layer', 'will', 'learn', 'nothing', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['if', 'you', 'set', 'your', 'probably', 'to', 'zero', 'then', 'all', 'neurons', 'are', 'kept', 'active', 'and', 'there', 'is', 'no', 'dropout', 'regularization', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], [\"it's\", 'pretty', 'much', 'just', 'a', 'more', 'computationally', 'costly', 'way', 'to', 'not', 'have', 'a', 'dropout', 'wrapper', 'at', 'all', 'because', 'you', 'still', 'have', 'to', 'roll', 'the', 'dice', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['of', 'course', 'somewhere', 'between', 'zero', 'and', 'one', 'is', 'where', 'you', 'want', 'to', 'be', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['specifically', 'with', 'dropout', 'probabilities', 'between', '10', 'to', '50', 'percent', 'where', 'a', 'good', 'baseline', 'is', 'usually', 'starting', 'at', '20', 'percent', 'and', 'then', 'adding', 'more', 'is', 'needed', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['there', 'is', 'no', 'one', 'size', 'fits', 'all', 'dropout', 'probability', 'for', 'all', 'models', 'and', 'all', 'data', 'distributions', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['dropout', 'acts', 'as', 'another', 'form', 'of', 'blank', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['it', 'forces', 'data', 'to', 'flow', 'down', 'blank', 'paths', 'so', 'that', 'there', 'is', 'a', 'more', 'even', 'spread', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['it', 'also', 'simulates', 'blank', 'learning', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], [\"don't\", 'forget', 'to', 'scale', 'the', 'dropout', 'activations', 'by', 'the', 'inverse', 'of', 'the', 'blank', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['we', 'remove', 'dropout', 'during', 'blank', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['the', 'correct', 'answer', 'is', 'e', 'dropout', 'act', 'is', 'another', 'form', 'of', 'regularization', 'so', 'the', 'model', 'can', 'generalize', 'better', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['it', 'does', 'this', 'turning', 'off', 'nodes', 'with', 'a', 'dropout', 'probability', 'which', 'forces', 'data', 'to', 'flow', 'down', 'multiple', 'paths', 'so', 'that', 'there', 'is', 'a', 'more', 'even', 'spread', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['otherwise', 'data', 'and', 'the', 'activations', 'associated', 'with', 'it', 'can', 'learn', 'to', 'take', 'preferential', 'paths', 'which', 'might', 'lead', 'to', 'under', 'training', 'of', 'the', 'network', 'as', 'a', 'whole', 'and', 'provide', 'poor', 'performance', 'on', 'new', 'data', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['dropout', 'also', 'simulates', 'ensemble', 'learning', 'by', 'creating', 'an', 'aggregate', 'of', 'two', 'to', 'the', 'n', 'models', 'due', 'to', 'the', 'random', 'turning', 'off', 'of', 'nodes', 'for', 'each', 'forward', 'pass', 'where', 'n', 'is', 'the', 'number', 'of', 'dropout', 'nodes', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['each', 'batch', 'sees', 'a', 'different', 'network', 'so', 'the', 'model', \"can't\", 'overfit', 'on', 'the', 'entire', 'training', 'set', 'much', 'like', 'a', 'random', 'forest', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], [\"don't\", 'forget', 'to', 'scale', 'the', 'dropout', 'activations', 'by', 'the', 'inverse', 'of', 'the', 'keep', 'probability', 'which', 'is', 'one', 'minus', 'the', 'dropout', 'probability', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['we', 'do', 'this', 'with', 'the', 'expectation', 'on', 'the', 'node', 'will', 'be', 'scaled', 'correctly', 'during', 'training', 'since', 'for', 'inference', 'it', 'will', 'always', 'be', 'on', 'since', 'we', 'remove', 'dropout', 'during', 'inference', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VJtmjrFdYUd"
      },
      "source": [
        "demo_sent_list = []\n",
        "for k,v in test_dict.items():\n",
        "  if k == \"document_embedding\":\n",
        "    demo_doc_embed = v\n",
        "  else: \n",
        "    demo_sent_list.append(v)"
      ],
      "execution_count": 465,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7I0hiyMdYWU"
      },
      "source": [
        "demo_sent_similarity = []\n",
        "for sent in demo_sent_list:\n",
        "  result = 1 - spatial.distance.cosine(demo_doc_embed, sent)\n",
        "  demo_sent_similarity.append(result)"
      ],
      "execution_count": 466,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2i8untrdYaX"
      },
      "source": [
        "demo_sent_index = [i for i in range(len(demo_sent_list))]"
      ],
      "execution_count": 467,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOQFTGKxdYcp"
      },
      "source": [
        "demo = pd.DataFrame()\n",
        "demo[\"Sentence_Number\"] = demo_sent_index\n",
        "demo[\"Similarity\"] = demo_sent_similarity\n"
      ],
      "execution_count": 468,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "tRgxJQDtdYfU",
        "outputId": "07196c63-a7e9-43b3-c3cc-deae9c3ef8a7"
      },
      "source": [
        "demo"
      ],
      "execution_count": 469,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence_Number</th>\n",
              "      <th>Similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.917691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.951502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.966761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.955667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.959113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>123</td>\n",
              "      <td>0.971756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>124</td>\n",
              "      <td>0.954405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>125</td>\n",
              "      <td>0.955035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>126</td>\n",
              "      <td>0.923079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>127</td>\n",
              "      <td>0.964274</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>128 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Sentence_Number  Similarity\n",
              "0                  0    0.917691\n",
              "1                  1    0.951502\n",
              "2                  2    0.966761\n",
              "3                  3    0.955667\n",
              "4                  4    0.959113\n",
              "..               ...         ...\n",
              "123              123    0.971756\n",
              "124              124    0.954405\n",
              "125              125    0.955035\n",
              "126              126    0.923079\n",
              "127              127    0.964274\n",
              "\n",
              "[128 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 469
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "rfRvPIPadj21",
        "outputId": "9f4f740b-cc0f-460a-ba17-3ff4e76d5c03"
      },
      "source": [
        "demo_sorted = demo.sort_values(\"Similarity\", ascending=False)\n",
        "demo_sorted"
      ],
      "execution_count": 470,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence_Number</th>\n",
              "      <th>Similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>89</td>\n",
              "      <td>0.986942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>54</td>\n",
              "      <td>0.985095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>46</td>\n",
              "      <td>0.983757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>0.980674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>96</td>\n",
              "      <td>0.980638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>36</td>\n",
              "      <td>0.861196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>65</td>\n",
              "      <td>0.858809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>120</td>\n",
              "      <td>0.820556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>118</td>\n",
              "      <td>0.813847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>57</td>\n",
              "      <td>0.810931</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>128 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Sentence_Number  Similarity\n",
              "89                89    0.986942\n",
              "54                54    0.985095\n",
              "46                46    0.983757\n",
              "24                24    0.980674\n",
              "96                96    0.980638\n",
              "..               ...         ...\n",
              "36                36    0.861196\n",
              "65                65    0.858809\n",
              "120              120    0.820556\n",
              "118              118    0.813847\n",
              "57                57    0.810931\n",
              "\n",
              "[128 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 470
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Kxaequgdj5S",
        "outputId": "4d11724a-b5db-4e53-8d7d-48d68874151a"
      },
      "source": [
        "len(doc_dict[122])"
      ],
      "execution_count": 473,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 473
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "id": "OpeDekU9dj7z",
        "outputId": "14b28c0f-17c4-429a-b7e6-289443192cf1"
      },
      "source": [
        "import re\n",
        "sent_idx_1 = demo_sorted.loc[1,\"Sentence_Number\"]\n",
        "sent_idx_2 = demo_sorted.loc[2,\"Sentence_Number\"]\n",
        "sent_idx_3 = demo_sorted.loc[3,\"Sentence_Number\"]\n",
        "\n",
        "final_sentences = test_doc[sent_idx_1] + test_doc[sent_idx_2] + test_doc[sent_idx_3]\n",
        "final_summary = \" \".join(final_sentences)\n",
        "final_summary_1 = re.sub(' {2,}', '. ', final_summary)\n",
        "print(\"Final Summary for the Document:\\n\")\n",
        "final_summary_1"
      ],
      "execution_count": 476,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final Summary for the Document:\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"let's sort of how we can train them some common pitfalls and something techniques to help speed up train and provide better journalism. in tensorflow using the estimator api using a dnnregressor is very similar to using a linearregressor with only a few parameters for the code that need to be added. we can use momentum based optimizers such as the default adagrad or we can try many others such as adam. \""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 476
        }
      ]
    }
  ]
}